Admin (to chat_manager):


find papers on LLM applications from arxiv in the last week, create a markdown table of different domains.


--------------------------------------------------------------------------------

Next speaker: Planner

Planner (to chat_manager):

### Initial Plan

**Objective:** To find recent papers on Large Language Model (LLM) applications from arXiv in the last week and create a markdown table categorizing them by different domains.

**Steps:**

1. **Search for Papers (Scientist)**
   - The scientist will use arXiv's search functionality to filter papers published in the last week that focus on LLM applications. This will involve using relevant keywords and possibly setting up alerts for new papers.

2. **Data Extraction (Scientist)**
   - The scientist will extract relevant information from the papers, such as title, authors, abstract, and domain of application (e.g., healthcare, education, finance, etc.).

3. **Data Organization (Engineer)**
   - The engineer will create a markdown table format to organize the extracted data. The table will include columns for the title, authors, abstract, and domain.

4. **Data Entry (Engineer)**
   - The engineer will input the extracted data into the markdown table, ensuring that it is well-formatted and easy to read.

5. **Review and Finalization (Scientist & Engineer)**
   - The scientist will review the markdown table for accuracy and completeness, while the engineer will ensure that the formatting is correct.

6. **Submission (Scientist)**
   - The scientist will submit the final markdown table to the admin for approval.

### Feedback Request
Please provide feedback on this plan, including any specific requirements or changes you would like to see.

--------------------------------------------------------------------------------

Next speaker: Critic

Critic (to chat_manager):

### Feedback on the Plan

Overall, the plan is well-structured and outlines a clear process for finding and organizing recent papers on LLM applications. However, there are a few areas where improvements can be made to enhance the effectiveness and verifiability of the output:

1. **Source URL Inclusion:**
   - It is essential to include verifiable information such as the source URL for each paper in the markdown table. This will allow users to easily access the original papers for further reading and verification. Consider adding a column for the URL in the markdown table.

2. **Keyword Specification:**
   - The plan mentions using "relevant keywords" for searching papers. It would be beneficial to specify which keywords will be used to ensure a focused search. For example, keywords like "Large Language Models," "GPT," "transformers," etc., could be included.

3. **Domain Categorization:**
   - While the plan mentions categorizing papers by domain, it would be helpful to define the specific domains that will be included in the table. This will provide clarity and ensure consistency in categorization.

4. **Data Extraction Details:**
   - The plan could benefit from specifying how the scientist will determine the domain of application for each paper. Will this be based on the abstract, keywords, or another method? Providing clarity on this process will enhance the reliability of the categorization.

5. **Review Process:**
   - The review process is a good step, but it might be useful to include a checklist of criteria for the scientist to use when reviewing the markdown table. This could include checking for completeness, accuracy of information, and proper formatting.

6. **Timeline:**
   - Consider adding a timeline for each step of the process. This will help in managing expectations and ensuring that the task is completed in a timely manner.

### Revised Plan Suggestion

**Objective:** To find recent papers on Large Language Model (LLM) applications from arXiv in the last week and create a markdown table categorizing them by different domains.

**Steps:**

1. **Search for Papers (Scientist)**
   - Use arXiv's search functionality with specific keywords (e.g., "Large Language Models," "GPT," "transformers") to filter papers published in the last week.

2. **Data Extraction (Scientist)**
   - Extract relevant information: title, authors, abstract, domain of application, and source URL.

3. **Data Organization (Engineer)**
   - Create a markdown table format with columns for title, authors, abstract, domain, and source URL.

4. **Data Entry (Engineer)**
   - Input the extracted data into the markdown table, ensuring proper formatting.

5. **Review and Finalization (Scientist & Engineer)**
   - Review the markdown table for accuracy and completeness, using a checklist for verification.

6. **Submission (Scientist)**
   - Submit the final markdown table to the admin for approval.

By incorporating these suggestions, the plan will be more robust, verifiable, and user-friendly.

--------------------------------------------------------------------------------

Next speaker: Scientist

Scientist (to chat_manager):

### Revised Plan

**Objective:** To find recent papers on Large Language Model (LLM) applications from arXiv in the last week and create a markdown table categorizing them by different domains.

**Steps:**

1. **Search for Papers (Scientist)**
   - Use arXiv's search functionality with specific keywords such as "Large Language Models," "GPT," "transformers," and "LLM applications" to filter papers published in the last week.

2. **Data Extraction (Scientist)**
   - Extract relevant information from each paper, including:
     - Title
     - Authors
     - Abstract
     - Domain of application (e.g., healthcare, education, finance, etc.)
     - Source URL

3. **Data Organization (Engineer)**
   - Create a markdown table format with the following columns:
     - Title
     - Authors
     - Abstract
     - Domain
     - Source URL

4. **Data Entry (Engineer)**
   - Input the extracted data into the markdown table, ensuring proper formatting and clarity.

5. **Review and Finalization (Scientist & Engineer)**
   - Review the markdown table for accuracy and completeness, using a checklist that includes:
     - Completeness of information
     - Accuracy of details
     - Proper formatting
     - Correct categorization of domains

6. **Submission (Scientist)**
   - Submit the final markdown table to the admin for approval.

### Timeline
- **Day 1:** Search for papers and extract data.
- **Day 2:** Organize data into markdown table format.
- **Day 3:** Review and finalize the table.
- **Day 4:** Submit the table for approval.

By following this revised plan, the process will be more structured, ensuring that the output is comprehensive, verifiable, and user-friendly.

--------------------------------------------------------------------------------

Next speaker: Engineer

Engineer (to chat_manager):

### Implementation Plan

Now that we have a clear plan, I will proceed to implement the data extraction and organization of recent papers on LLM applications from arXiv. Below is the Python script that will perform the search and create the markdown table.

```python
import requests
from datetime import datetime, timedelta

# Define the search parameters
keywords = "Large Language Models OR GPT OR transformers OR LLM applications"
end_date = datetime.now()
start_date = end_date - timedelta(days=7)

# Format dates for arXiv API
start_date_str = start_date.strftime('%Y%m%d')
end_date_str = end_date.strftime('%Y%m%d')

# arXiv API URL
url = f"http://export.arxiv.org/api/query?search_query=all:{keywords}&start=0&max_results=100&sortBy=submittedDate&sortOrder=descending"

# Fetch the data from arXiv
response = requests.get(url)
data = response.text

# Parse the data
import xml.etree.ElementTree as ET

root = ET.fromstring(data)
entries = root.findall('{http://www.w3.org/2005/Atom}entry')

# Prepare markdown table
markdown_table = "| Title | Authors | Abstract | Domain | Source URL |\n"
markdown_table += "|-------|---------|----------|--------|------------|\n"

# Extract relevant information
for entry in entries:
    title = entry.find('{http://www.w3.org/2005/Atom}title').text
    authors = ', '.join([author.find('{http://www.w3.org/2005/Atom}name').text for author in entry.findall('{http://www.w3.org/2005/Atom}author')])
    abstract = entry.find('{http://www.w3.org/2005/Atom}summary').text
    domain = "General"  # Placeholder for domain categorization
    source_url = entry.find('{http://www.w3.org/2005/Atom}id').text

    # Add to markdown table
    markdown_table += f"| {title} | {authors} | {abstract} | {domain} | [Link]({source_url}) |\n"

# Output the markdown table
print(markdown_table)
```

### Execution Instructions
1. Run the script in a Python environment with internet access.
2. The script will fetch the latest papers from arXiv based on the specified keywords and create a markdown table.
3. Review the output for accuracy and completeness.

### Note
- The domain categorization is currently set to "General" as a placeholder. You may need to manually categorize the papers based on their abstracts or content after reviewing the output.
- Ensure that you have the `requests` library installed in your Python environment. If not, you can install it using `pip install requests`.

--------------------------------------------------------------------------------

Next speaker: Executor


>>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...
Executor (to chat_manager):

exitcode: 0 (execution succeeded)
Code output: 
| Title | Authors | Abstract | Domain | Source URL |
|-------|---------|----------|--------|------------|
| The VIRUS-dE Survey I: Stars in dwarf elliptical galaxies - 3D dynamics
  and radially resolved stellar initial mass functions | Mathias Lipka, Jens Thomas, Roberto Saglia, Ralf Bender, Maximilian Fabricius, Gary J. Hill, Matthias Kluge, Martin Landriau, Ximena Mazzalay, Eva Noyola, Taniya Parikh, Jan Snigula |   We analyse the stellar structure of a sample of dwarf ellipticals (dE)
inhabiting various environments within the Virgo cluster. Integral-field
observations with a high spectral resolution allow us to robustly determine
their low velocity dispersions ($\sim25$ km s$^{-1}$) and higher-order
kinematic moments out to the half-light radius. We find the dEs exhibit a
diversity in ages with the younger dEs being less enhanced than the older,
suggesting a complex star formation history for those dEs that recently entered
Virgo while others have been quenched shortly after reionization.
Orbit-superposition modeling allowed us to recover viewing angles, stellar
mass-to-light ratios (with gradients), as well as the intrinsic orbit
structure. We find that the angular momentum of the dEs is strongly suppressed
compared to ordinary early-type galaxies and correlates with the environment.
Flattened dEs are so because of a suppressed kinetic energy perpendicular to
their equatorial plane. Combining population and dynamical modeling results, we
find an age-dependent stellar initial mass function (IMF) or, alternatively,
evidence for a more extended star formation history for those galaxies that
have had higher initial mass and/or inhabited lower density environments. dEs
appear to have a spatially homogeneous stellar structure but the state they
were `frozen' in as they stopped forming stars varies dramatically according to
their initial conditions.
 | General | [Link](http://arxiv.org/abs/2409.10518v1) |
| RetrievalAttention: Accelerating Long-Context LLM Inference via Vector
  Retrieval | Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu |   Transformer-based large Language Models (LLMs) become increasingly important
in various domains. However, the quadratic time complexity of attention
operation poses a significant challenge for scaling to longer contexts due to
the extremely high inference latency and GPU memory consumption for caching
key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free
approach to accelerate attention computation. To leverage the dynamic sparse
property of attention, RetrievalAttention builds approximate nearest neighbor
search (ANNS) indexes upon KV vectors in CPU memory and retrieves the most
relevant ones via vector search during generation. Due to the
out-of-distribution (OOD) between query vectors and key vectors, off-the-shelf
ANNS indexes still need to scan O(N) (usually 30% of all keys) data for
accurate retrieval, which fails to exploit the high sparsity.
RetrievalAttention first identifies the OOD challenge of ANNS-based attention,
and addresses it via an attention-aware vector search algorithm that can adapt
to queries and only access 1--3% of data, thus achieving a sub-linear time
complexity. RetrievalAttention greatly reduces the inference cost of
long-context LLM with much lower GPU memory requirements while maintaining the
model accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for
serving 128K tokens in LLMs with 8B parameters, which is capable of generating
one token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).
 | General | [Link](http://arxiv.org/abs/2409.10516v1) |
| An Efficient Self-Learning Framework For Interactive Spoken Dialog
  Systems | Hitesh Tulsiani, David M. Chan, Shalini Ghosh, Garima Lalwani, Prabhat Pandey, Ankish Bansal, Sri Garimella, Ariya Rastrow, Björn Hoffmeister |   Dialog systems, such as voice assistants, are expected to engage with users
in complex, evolving conversations. Unfortunately, traditional automatic speech
recognition (ASR) systems deployed in such applications are usually trained to
recognize each turn independently and lack the ability to adapt to the
conversational context or incorporate user feedback. In this work, we introduce
a general framework for ASR in dialog systems that can go beyond learning from
single-turn utterances and learn over time how to adapt to both explicit
supervision and implicit user feedback present in multi-turn conversations. We
accomplish that by leveraging advances in student-teacher learning and
context-aware dialog processing, and designing contrastive self-supervision
approaches with Ohm, a new online hard-negative mining approach. We show that
leveraging our new framework compared to traditional training leads to relative
WER reductions of close to 10% in real-world dialog systems, and up to 26% on
public synthetic data.
 | General | [Link](http://arxiv.org/abs/2409.10515v1) |
| Constraints on axions from patchy screening of the cosmic microwave
  background | Samuel Goldstein, Fiona McCarthy, Cristina Mondino, J. Colin Hill, Junwu Huang, Matthew C. Johnson |   The resonant conversion of cosmic microwave background (CMB) photons into
axions within large-scale structure induces an anisotropic spectral distortion
in CMB temperature maps. Applying state-of-the-art foreground cleaning
techniques to $\textit{Planck}$ CMB observations, we construct maps of
axion-induced "patchy screening" of the CMB. We cross-correlate these maps with
data from the $\textit{unWISE}$ galaxy survey and find no evidence of axions.
We constrain the axion-photon coupling, $g_{a\gamma\gamma} \lesssim 2 \times
10^{-12}~{\rm GeV}^{-1}$, at the 95% confidence level for axion masses in the
range $10^{-13}~{\rm eV} \lesssim m_a \lesssim 10^{-12}~{\rm eV}$. These
constraints are competitive with the tightest astrophysical axion limits in
this mass range and are inferred from robust population-level statistics, which
makes them complementary to existing searches that rely on modeling of
individual systems.
 | General | [Link](http://arxiv.org/abs/2409.10514v1) |
| Weak Superimposed Codes of Improved Asymptotic Rate and Their Randomized
  Construction | Yu Tsunoda, Yuichiro Fujiwara |   Weak superimposed codes are combinatorial structures related closely to
generalized cover-free families, superimposed codes, and disjunct matrices in
that they are only required to satisfy similar but less stringent conditions.
This class of codes may also be seen as a stricter variant of what are known as
locally thin families in combinatorics. Originally, weak superimposed codes
were introduced in the context of multimedia content protection against illegal
distribution of copies under the assumption that a coalition of malicious users
may employ the averaging attack with adversarial noise. As in many other kinds
of codes in information theory, it is of interest and importance in the study
of weak superimposed codes to find the highest achievable rate in the
asymptotic regime and give an efficient construction that produces an infinite
sequence of codes that achieve it. Here, we prove a tighter lower bound than
the sharpest known one on the rate of optimal weak superimposed codes and give
a polynomial-time randomized construction algorithm for codes that
asymptotically attain our improved bound with high probability. Our
probabilistic approach is versatile and applicable to many other related codes
and arrays.
 | General | [Link](http://arxiv.org/abs/2409.10511v1) |
| Pennsieve - A Collaborative Platform for Translational Neuroscience and
  Beyond | Zack Goldblum, Zhongchuan Xu, Haoer Shi, Patryk Orzechowski, Jamaal Spence, Kathryn A Davis, Brian Litt, Nishant Sinha, Joost Wagenaar |   The exponential growth of neuroscientific data necessitates platforms that
facilitate data management and multidisciplinary collaboration. In this paper,
we introduce Pennsieve - an open-source, cloud-based scientific data management
platform built to meet these needs. Pennsieve supports complex multimodal
datasets and provides tools for data visualization and analyses. It takes a
comprehensive approach to data integration, enabling researchers to define
custom metadata schemas and utilize advanced tools to filter and query their
data. Pennsieve's modular architecture allows external applications to extend
its capabilities, and collaborative workspaces with peer-reviewed data
publishing mechanisms promote high-quality datasets optimized for downstream
analysis, both in the cloud and on-premises.
  Pennsieve forms the core for major neuroscience research programs including
the NIH SPARC Initiative, NIH HEAL Initiative's PRECISION Human Pain Network,
and NIH HEAL RE-JOIN Initiative. It serves more than 80 research groups
worldwide, along with several large-scale, inter-institutional projects at
clinical sites through the University of Pennsylvania. Underpinning the
SPARC.Science, Epilepsy.Science, and Pennsieve Discover portals, Pennsieve
stores over 125 TB of scientific data, with 35 TB of data publicly available
across more than 350 high-impact datasets. It adheres to the findable,
accessible, interoperable, and reusable (FAIR) principles of data sharing and
is recognized as one of the NIH-approved Data Repositories. By facilitating
scientific data management, discovery, and analysis, Pennsieve fosters a robust
and collaborative research ecosystem for neuroscience and beyond.
 | General | [Link](http://arxiv.org/abs/2409.10509v1) |
| General-relativistic resistive-magnetohydrodynamics simulations of
  self-consistent magnetized rotating neutron stars | Patrick Chi-Kit Cheong, Antonios Tsokaros, Milton Ruiz, Fabrizio Venturi, Juno Chun Lung Chan, Anson Ka Long Yip, Koji Uryu |   We present the first general-relativistic resistive magnetohydrodynamics
simulations of self-consistent, rotating neutron stars with mixed poloidal and
toroidal magnetic fields. Specifically, we investigate the role of resistivity
in the dynamical evolution of neutron stars over a period of up to 100 ms and
its effects on their quasi-equilibrium configurations. Our results demonstrate
that resistivity can significantly influence the development of
magnetohydrodynamic instabilities, resulting in markedly different magnetic
field geometries. Additionally, resistivity suppresses the growth of these
instabilities, leading to a reduction in the amplitude of emitted gravitational
waves. Despite the variations in magnetic field geometries, the ratio of
poloidal to toroidal field energies remains consistently 9:1 throughout the
simulations, for the models we investigated.
 | General | [Link](http://arxiv.org/abs/2409.10508v1) |
| Beth-Uhlenbeck equation for the thermodynamics of fluctuations in a
  generalised 2+1D Gross-Neveu model | Biplab Mahato, David Blaschke, Dietmar Ebert |   We study a generalised version of Gross-Neveu model in 2+1 dimensions. The
model is inspired from graphene which shows a linear dispersion relation near
the Dirac points. The phase structure and the thermodynamic properties in the
mean field approximation have been studied before. Here we go beyond the mean
field level by deriving a Beth-Uhlenbeck equation for Gaussian fluctuations,
solutions of which we explore numerically, for the first time including their
momentum dependence. We discuss the excitonic mass, fluctuation pressure and
phase shifts. We also perform a comparison with the NJL model in 3+1 dimension
and discuss its implication for graphene.
 | General | [Link](http://arxiv.org/abs/2409.10507v1) |
| Context-aware Code Segmentation for C-to-Rust Translation using Large
  Language Models | Momoko Shiraishi, Takahiro Shinagawa |   There is strong motivation to translate C code into Rust code due to the
continuing threat of memory safety vulnerabilities in existing C programs and
the significant attention paid to Rust as an alternative to the C language.
While large language models (LLMs) show promise for automating this translation
by generating more natural and safer code than rule-based methods, previous
studies have shown that LLM-generated Rust code often fails to compile, even
for relatively small C programs, due to significant differences between the two
languages and context window limitations. We propose an LLM-based translation
scheme that improves the success rate of translating large-scale C code into
compilable Rust code. Our approach involves three key techniques: (1)
pre-processing the C code to better align its structure and expressions with
Rust, (2) segmenting the code into optimally sized translation units to avoid
exceeding the LLM's context window limits, and (3) iteratively compiling and
repairing errors while maintaining consistency between translation units using
context-supplementing prompts. Compilation success is an essential first step
in achieving functional equivalence, as only compilable code can be further
tested. In experiments with 20 benchmark C programs, including those exceeding
4 kilo lines of code, we successfully translated all programs into compilable
Rust code without losing corresponding parts of the original code.
 | General | [Link](http://arxiv.org/abs/2409.10506v1) |
| Machine Learning Optimization of non-Kasha Behavior and of Transient
  Dynamics in Model Retinal Isomerization | Davinder Singh, Chern Chuang, Paul Brumer |   Designing a model of retinal isomerization in Rhodopsin, the first step in
vision, that accounts for both experimental transient and stationary state
observables is challenging. Here, multi-objective Bayesian optimization is
employed to refine the parameters of a minimal two-state-two-mode (TM) model
describing the photoisomerization of retinal in Rhodopsin. With an appropriate
selection of objectives, the optimized retinal model predicts excitation
wavelength-dependent fluorescence spectra that closely align with
experimentally observed non-Kasha behavior in the non-equilibrium steady state.
Further, adjustments to the potential energy surface within the TM model reduce
the discrepancies across the time domain. Overall, agreement with experimental
data is excellent.
 | General | [Link](http://arxiv.org/abs/2409.10505v1) |
| DILA: Dictionary Label Attention for Mechanistic Interpretability in
  High-dimensional Multi-label Medical Coding Prediction | John Wu, David Wu, Jimeng Sun |   Predicting high-dimensional or extreme multilabels, such as in medical
coding, requires both accuracy and interpretability. Existing works often rely
on local interpretability methods, failing to provide comprehensive
explanations of the overall mechanism behind each label prediction within a
multilabel set. We propose a mechanistic interpretability module called
DIctionary Label Attention (\method) that disentangles uninterpretable dense
embeddings into a sparse embedding space, where each nonzero element (a
dictionary feature) represents a globally learned medical concept. Through
human evaluations, we show that our sparse embeddings are more human
understandable than its dense counterparts by at least 50 percent. Our
automated dictionary feature identification pipeline, leveraging large language
models (LLMs), uncovers thousands of learned medical concepts by examining and
summarizing the highest activating tokens for each dictionary feature. We
represent the relationships between dictionary features and medical codes
through a sparse interpretable matrix, enhancing the mechanistic and global
understanding of the model's predictions while maintaining competitive
performance and scalability without extensive human annotation.
 | General | [Link](http://arxiv.org/abs/2409.10504v1) |
| Causal Language Modeling Can Elicit Search and Reasoning Capabilities on
  Logic Puzzles | Kulin Shah, Nishanth Dikkala, Xin Wang, Rina Panigrahy |   Causal language modeling using the Transformer architecture has yielded
remarkable capabilities in Large Language Models (LLMs) over the last few
years. However, the extent to which fundamental search and reasoning
capabilities emerged within LLMs remains a topic of ongoing debate. In this
work, we study if causal language modeling can learn a complex task such as
solving Sudoku puzzles. To solve a Sudoku, the model is first required to
search over all empty cells of the puzzle to decide on a cell to fill and then
apply an appropriate strategy to fill the decided cell. Sometimes, the
application of a strategy only results in thinning down the possible values in
a cell rather than concluding the exact value of the cell. In such cases,
multiple strategies are applied one after the other to fill a single cell. We
observe that Transformer models trained on this synthetic task can indeed learn
to solve Sudokus (our model solves $94.21\%$ of the puzzles fully correctly)
when trained on a logical sequence of steps taken by a solver. We find that
training Transformers with the logical sequence of steps is necessary and
without such training, they fail to learn Sudoku. We also extend our analysis
to Zebra puzzles (known as Einstein puzzles) and show that the model solves
$92.04 \%$ of the puzzles fully correctly. In addition, we study the internal
representations of the trained Transformer and find that through linear
probing, we can decode information about the set of possible values in any
given cell from them, pointing to the presence of a strong reasoning engine
implicit in the Transformer weights.
 | General | [Link](http://arxiv.org/abs/2409.10502v1) |
| Alignment with nonlinear velocity couplings: collision-avoidance and
  micro-to-macro mean-field limits | Young-Pil Choi, Michał Fabisiak, Jan Peszek |   We investigate the pressureless fractional Euler-alignment system with
nonlinear velocity couplings, referred to as the $p$-Euler-alignment system.
This model features a nonlinear velocity alignment force, interpreted as a
density-weighted fractional $p$-Laplacian when the singularity parameter
$\alpha$ exceeds the spatial dimension $d$. Our primary goal is to establish
the existence of solutions for strongly singular interactions ($\alpha \ge d$)
and compactly supported initial conditions. We construct solutions as
mean-field limits of empirical measures from a kinetic variant of the
$p$-Euler-alignment system. Specifically, we show that a sequence of empirical
measures converges to a finite Radon measure, whose local density and velocity
satisfy the $p$-Euler-alignment system. Our results are the first to prove the
existence of solutions to this system in multi-dimensional settings without
significant initial data restrictions, covering both nonlinear ($p>2$) and
linear ($p=2$) cases. Additionally, we establish global existence, uniqueness,
and collision avoidance for the corresponding particle ODE system under
non-collisional initial conditions, extending previous results for $1 \le p \le
\alpha + 2$. This analysis supports our mean-field limit argument and
contributes to understanding alignment models with singular communication.
 | General | [Link](http://arxiv.org/abs/2409.10501v1) |
| Focus diverse phase retrieval test results on broadband continuous
  wavefront sensing in space telescope applications | Hyukmo Kang, Kyle Van Gorkom, Meghdoot Biswas, Daewook Kim, Ewan S. Douglas |   Continuous wavefront sensing benefits space observatories in on-orbit optical
performance maintenance. To measure the phase of a wavefront, phase retrieval
is an attractive technique as it uses multiple point spread function (PSF)
images that are acquired by the telescope itself without extra metrology
systems nor complicated calibration. The focus diverse phase retrieval utilizes
PSFs from predetermined defocused positions to enhance the dynamic range of the
algorithm. We describe an updated visible light active optics testbed with the
addition of a linear motorized focus stage. The performance of the phase
retrieval algorithm in broadband is tested under various cases. While broadband
pass filters have advantages in higher signal-to-noise ratio (SNR), the
performance of phase retrieval can be restricted due to blurred image caused by
diffraction and increased computing cost. We used multiple bandpass filters (10
nm, 88 nm, and 150 nm) and investigated effects of bandwidth on the accuracy
and required image acquisition conditions such as SNR, reaching accuracies
below 20 nm RMS wavefront error at the widest bandwidth. We also investigated
the dynamic range of the phase retrieval algorithm depending on the bandwidth
and required amount of defocus to expand dynamic range. Finally, we simulated
the continuous wavefront sensing and correction loop with a range of
statistically generated representative telescope disturbance time series to
test for edge cases.
 | General | [Link](http://arxiv.org/abs/2409.10500v1) |
| The role of higher-order terms in trapped-ion quantum computing with
  magnetic gradient induced coupling | Sebastian Nagies, Kevin T. Geier, Javed Akram, Junichi Okamoto, Dimitris Badounas, Christof Wunderlich, Michael Johanning, Philipp Hauke |   Trapped-ion hardware based on the Magnetic Gradient Induced Coupling (MAGIC)
scheme is emerging as a promising platform for quantum computing. Nevertheless,
in this (as in any other) quantum-computing platform, many technical questions
still have to be resolved before large-scale and error-tolerant applications
are possible. In this work, we present a thorough discussion of the
contribution of higher-order terms to the MAGIC setup, which can occur due to
anharmonicities in the external potential of the ion crystal (e.g., through
Coulomb repulsion) or through curvature of the applied magnetic field. These
terms take the form of three-spin couplings as well as diverse terms that
couple spins to phonons. We find that most of these are negligible in realistic
situations, with only two contributions that need careful attention. First,
there are parasitic longitudinal fields whose strength increases with chain
length, but which can easily be compensated by a microwave detuning. Second,
anharmonicities of the Coulomb interaction can lead to well-known two-to-one
conversions of phonon excitations, which can be avoided if the phonons are
ground-state cooled. Our detailed analysis constitutes an important
contribution on the way of making magnetic-gradient trapped-ion quantum
technology fit for large-scale applications, and it may inspire new ways to
purposefully design interaction terms.
 | General | [Link](http://arxiv.org/abs/2409.10498v1) |
| MusicLIME: Explainable Multimodal Music Understanding | Theodoros Sotirou, Vassilis Lyberatos, Orfeas Menis Mastromichalakis, Giorgos Stamou |   Multimodal models are critical for music understanding tasks, as they capture
the complex interplay between audio and lyrics. However, as these models become
more prevalent, the need for explainability grows-understanding how these
systems make decisions is vital for ensuring fairness, reducing bias, and
fostering trust. In this paper, we introduce MusicLIME, a model-agnostic
feature importance explanation method designed for multimodal music models.
Unlike traditional unimodal methods, which analyze each modality separately
without considering the interaction between them, often leading to incomplete
or misleading explanations, MusicLIME reveals how audio and lyrical features
interact and contribute to predictions, providing a holistic view of the
model's decision-making. Additionally, we enhance local explanations by
aggregating them into global explanations, giving users a broader perspective
of model behavior. Through this work, we contribute to improving the
interpretability of multimodal music models, empowering users to make informed
choices, and fostering more equitable, fair, and transparent music
understanding systems.
 | General | [Link](http://arxiv.org/abs/2409.10496v1) |
| Incorporating Classifier-Free Guidance in Diffusion Model-Based
  Recommendation | Noah Buchanan, Susan Gauch, Quan Mai |   This paper presents a diffusion-based recommender system that incorporates
classifier-free guidance. Most current recommender systems provide
recommendations using conventional methods such as collaborative or
content-based filtering. Diffusion is a new approach to generative AI that
improves on previous generative AI approaches such as Variational Autoencoders
(VAEs) and Generative Adversarial Networks (GANs). We incorporate diffusion in
a recommender system that mirrors the sequence users take when browsing and
rating items. Although a few current recommender systems incorporate diffusion,
they do not incorporate classifier-free guidance, a new innovation in diffusion
models as a whole. In this paper, we present a diffusion recommender system
that augments the underlying recommender system model for improved performance
and also incorporates classifier-free guidance. Our findings show improvements
over state-of-the-art recommender systems for most metrics for several
recommendation tasks on a variety of datasets. In particular, our approach
demonstrates the potential to provide better recommendations when data is
sparse.
 | General | [Link](http://arxiv.org/abs/2409.10494v1) |
| Renormalization of the Einstein-Cartan Theory in First-Order Form | F. T. Brandt, J. Frenkel, S. Martins-Filho, D. G. C. McKeon |   We examine the Einstein-Cartan (EC) theory in first-order form, which has a
diffeomorphism as well as a local Lorentz invariance. We study the
renormalizability of this theory in the framework of the Batalin-Vilkovisky
formalism, which allows for a gauge invariant renormalization. Using the
background field method, we discuss the gauge invariance of the background
effective action and analyze the Ward identities which reflect the symmetries
of the EC theory. As an application, we compute, in a general background gauge,
the self-energy of the tetrad field at one-loop order.
 | General | [Link](http://arxiv.org/abs/2409.10493v1) |
| What we can learn from the angular differential rates (only) in
  semileptonic $B \to D^* \ell ν_\ell$ decays | G. Martinelli, S. Simula, L. Vittorio |   We present a new, simple approach to the study of semileptonic $B \to D^*
\ell \nu_\ell$ decays based on the angular distributions of the final state
particles only. Our approach is model independent and never requires the
knowledge of $\vert V_{cb}\vert$. By studying such distributions in the case of
light leptons, a comparison between results from different data sets from the
Belle and BelleII Collaborations and between data and Standard Model
calculations is also given for several interesting quantities. A good
consistency is observed between some of the experimental results and the
theoretical predictions.
 | General | [Link](http://arxiv.org/abs/2409.10492v1) |
| Code Vulnerability Detection: A Comparative Analysis of Emerging Large
  Language Models | Shaznin Sultana, Sadia Afreen, Nasir U. Eisty |   The growing trend of vulnerability issues in software development as a result
of a large dependence on open-source projects has received considerable
attention recently. This paper investigates the effectiveness of Large Language
Models (LLMs) in identifying vulnerabilities within codebases, with a focus on
the latest advancements in LLM technology. Through a comparative analysis, we
assess the performance of emerging LLMs, specifically Llama, CodeLlama, Gemma,
and CodeGemma, alongside established state-of-the-art models such as BERT,
RoBERTa, and GPT-3. Our study aims to shed light on the capabilities of LLMs in
vulnerability detection, contributing to the enhancement of software security
practices across diverse open-source repositories. We observe that CodeGemma
achieves the highest F1-score of 58\ and a Recall of 87\, amongst the recent
additions of large language models to detect software security vulnerabilities.
 | General | [Link](http://arxiv.org/abs/2409.10490v1) |
| Flash STU: Fast Spectral Transform Units | Y. Isabel Liu, Windsor Nguyen, Yagiz Devre, Evan Dogariu, Anirudha Majumdar, Elad Hazan |   This paper describes an efficient, open source PyTorch implementation of the
Spectral Transform Unit. We investigate sequence prediction tasks over several
modalities including language, robotics, and simulated dynamical systems. We
find that for the same parameter count, the STU and its variants outperform the
Transformer as well as other leading state space models across various
modalities.
 | General | [Link](http://arxiv.org/abs/2409.10489v1) |
| Do Pre-trained Vision-Language Models Encode Object States? | Kaleb Newman, Shijie Wang, Yuan Zang, David Heffren, Chen Sun |   For a vision-language model (VLM) to understand the physical world, such as
cause and effect, a first step is to capture the temporal dynamics of the
visual world, for example how the physical states of objects evolve over time
(e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs
pre-trained on web-scale data learn to encode object states, which can be
extracted with zero-shot text prompts. We curate an object state recognition
dataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models
trained with contrastive and generative objectives. We observe that while these
state-of-the-art vision-language models can reliably perform object
recognition, they consistently fail to accurately distinguish the objects'
physical states. Through extensive experiments, we identify three areas for
improvements for VLMs to better encode object states, namely the quality of
object localization, the architecture to bind concepts to objects, and the
objective to learn discriminative visual and language encoders on object
states. Data and code are released.
 | General | [Link](http://arxiv.org/abs/2409.10488v1) |
| A Host Galaxy Morphology Link Between Quasi-Periodic Eruptions and Tidal
  Disruption Events | Olivier Gilbert, John J. Ruan, Michael Eracleous, Daryl Haggard, Jessie C. Runnoe |   The physical processes that produce X-ray Quasi-Periodic Eruptions (QPEs)
recently discovered from the nuclei of several low-redshift galaxies are
mysterious. Several pieces of observational evidence strongly suggest a link
between QPEs and Tidal Disruption Events (TDE). Previous studies also reveal
that the morphologies of TDE host galaxies are highly concentrated, with high
Sersic indicies, bulge-to-total light (B/T) ratios, and stellar surface mass
densities relative to the broader galaxy population. We use these distinctive
properties to test the link between QPEs and TDEs, by comparing these
parameters of QPE host galaxies to TDE host galaxies. We employ archival Legacy
Survey images of a sample of 9 QPE host galaxies and a sample of 13 TDE host
galaxies, and model their surface brightness profiles. We show that QPE host
galaxies have high Sersic indices of ~3, high B/T ratios of ~0.5, and high
surface mass densities of ~10^10 Msun kpc^-2. These properties are similar to
TDE host galaxies, but are in strong contrast to a mass- and redshift-matched
control sample of galaxies. We also find tentative evidence that the central
black holes in both QPE and TDE host galaxies are undermassive relative to
their stellar mass. The morphological similarities between QPE and TDE host
galaxies at the population level add to the mounting evidence of a physical
link between these phenomena, and favor QPE models that also invoke TDEs.
 | General | [Link](http://arxiv.org/abs/2409.10486v1) |
| Tuning the Rate of Tightly Packed Systems To Produce Planet Occurrence
  Trends with Galactic Height | Sarah Ballard |   The formation of planetary systems has historically been considered in
isolation, decoupled from processes on galactic scales. Recent findings
employing data from ESA's Gaia mission challenge this narrative, identifying
trends in planet occurrence with galactic kinematics and stellar age. The
findings indicate changes in planet occurrence over and above the predicted
changes from metallicity variation within the Milky Way, so that changes to
stellar metallicity alone (long understood to be deterministic in planet
outcomes) cannot explain the trends entirely. The scope of potential factors
influencing planet formation has grown progressively wider, with accompanying
theoretical support for galactic-scale influences upon planet formation. In
this manuscript, we investigate specifically how changes to the rate of Systems
of Tightly-packed Inner Planets (STIPs) could manifest as a trend in planet
occurrence with galactic height. We focus our study upon M dwarf planetary
systems for two reasons: first, they host STIPs at high rates, and secondly,
their longevity makes them useful probes for kinematic trends over Gyr. We
consider two models for a varying STIP rate: one in which STIP likelihood is
determined by stellar age alone, irrespective of galactic time, and another in
which the STIP likelihood suddenly increased in recent galactic history. Both
models, which impose a higher STIP likelihood among younger stars, produce a
negative gradient in planet occurrence with increasing height from the galactic
midplane. We find that a step function model in which STIP likelihood increased
by a factor of several ~a few Gyr ago resembles an observed trend among FGK
dwarfs. We consider plausible physical mechanisms that could mimic the
hypothesized model, given known links between STIP occurrence and other stellar
and planetary properties.
 | General | [Link](http://arxiv.org/abs/2409.10485v1) |
| XLM for Autonomous Driving Systems: A Comprehensive Review | Sonda Fourati, Wael Jaafar, Noura Baccar, Safwan Alfattani |   Large Language Models (LLMs) have showcased remarkable proficiency in various
information-processing tasks. These tasks span from extracting data and
summarizing literature to generating content, predictive modeling,
decision-making, and system controls. Moreover, Vision Large Models (VLMs) and
Multimodal LLMs (MLLMs), which represent the next generation of language
models, a.k.a., XLMs, can combine and integrate many data modalities with the
strength of language understanding, thus advancing several information-based
systems, such as Autonomous Driving Systems (ADS). Indeed, by combining
language communication with multimodal sensory inputs, e.g., panoramic images
and LiDAR or radar data, accurate driving actions can be taken. In this
context, we provide in this survey paper a comprehensive overview of the
potential of XLMs towards achieving autonomous driving. Specifically, we review
the relevant literature on ADS and XLMs, including their architectures, tools,
and frameworks. Then, we detail the proposed approaches to deploy XLMs for
autonomous driving solutions. Finally, we provide the related challenges to XLM
deployment for ADS and point to future research directions aiming to enable XLM
adoption in future ADS frameworks.
 | General | [Link](http://arxiv.org/abs/2409.10484v1) |
| Efficient polarizable QM/MM using the direct reaction field Hamiltonian
  with electrostatic potential fitted multipole operators | Thomas P. Fay, Nicolas Ferré, Miquel Huix-Rotllant |   Electronic polarization and dispersion are decisive actors in determining
interaction energies between molecules. These interactions have a particularly
profound effect on excitation energies of molecules in complex environments,
especially when the excitation involves a significant degree of charge
reorganisation. The direct reaction field (DRF) approach, which has seen a
recent revival of interest, provides a powerful framework for describing these
interactions in quantum mechanics/molecular mechanics (QM/MM) models of
systems, where a small subsystem of interest is described using quantum
chemical methods and the remainder is treated with a simple MM force field. In
this paper we show how the DRF approach can be combined with the electrostatic
potential fitted (ESPF) multipole operator description of the QM region charge
density, which reduces the scaling $\mathcal{O}(N_\mathrm{MM}^3)$ of the method
with MM system to $\mathcal{O}(N_\mathrm{MM}^2)$. We also show how the DRF
approach can be combined with fluctuating charge descriptions of the
polarizable environment, as well as previously used atom-centred
dipole-polarizability based models. We further show that the ESPF-DRF method
provides an accurate description of molecular interactions in both ground and
excited electronic states of the QM system and apply it to predict the gas to
aqueous solution solvatochromic shifts in the UV/visible absorption spectrum of
acrolein.
 | General | [Link](http://arxiv.org/abs/2409.10483v1) |
| Schrodinger's Memory: Large Language Models | Wei Wang, Qing Li |   Memory is the foundation of LLMs' functionality, yet past research has lacked
an in-depth exploration of their memory capabilities and underlying theory. In
this paper, we apply UAT theory to explain the memory mechanism of LLMs and
propose a new approach for evaluating LLM performance by comparing the memory
capacities of different models. Through extensive experiments, we validate our
theory and the memory abilities of LLMs. Finally, we compare the capabilities
of the human brain and LLMs, highlighting both their similarities and
differences in terms of working mechanisms.
 | General | [Link](http://arxiv.org/abs/2409.10482v1) |
| Exploring 3D Face Reconstruction and Fusion Methods for Face
  Verification: A Case-Study in Video Surveillance | Simone Maurizio La Cava, Sara Concas, Ruben Tolosana, Roberto Casula, Giulia Orrù, Martin Drahansky, Julian Fierrez, Gian Luca Marcialis |   3D face reconstruction (3DFR) algorithms are based on specific assumptions
tailored to distinct application scenarios. These assumptions limit their use
when acquisition conditions, such as the subject's distance from the camera or
the camera's characteristics, are different than expected, as typically happens
in video surveillance. Additionally, 3DFR algorithms follow various strategies
to address the reconstruction of a 3D shape from 2D data, such as statistical
model fitting, photometric stereo, or deep learning. In the present study, we
explore the application of three 3DFR algorithms representative of the SOTA,
employing each one as the template set generator for a face verification
system. The scores provided by each system are combined by score-level fusion.
We show that the complementarity induced by different 3DFR algorithms improves
performance when tests are conducted at never-seen-before distances from the
camera and camera characteristics (cross-distance and cross-camera settings),
thus encouraging further investigations on multiple 3DFR-based approaches.
 | General | [Link](http://arxiv.org/abs/2409.10481v1) |
| Addressing misspecification in contextual optimization | Omar Bennouna, Jiawei Zhang, Saurabh Amin, Asuman Ozdaglar |   We study a linear contextual optimization problem where a decision maker has
access to historical data and contextual features to learn a cost prediction
model aimed at minimizing decision error. We adopt the predict-then-optimize
framework for this analysis. Given that perfect model alignment with reality is
often unrealistic in practice, we focus on scenarios where the chosen
hypothesis set is misspecified. In this context, it remains unclear whether
current contextual optimization approaches can effectively address such model
misspecification. In this paper, we present a novel integrated learning and
optimization approach designed to tackle model misspecification in contextual
optimization. This approach offers theoretical generalizability, tractability,
and optimality guarantees, along with strong practical performance. Our method
involves minimizing a tractable surrogate loss that aligns with the performance
value from cost vector predictions, regardless of whether the model
misspecified or not, and can be optimized in reasonable time. To our knowledge,
no previous work has provided an approach with such guarantees in the context
of model misspecification.
 | General | [Link](http://arxiv.org/abs/2409.10479v1) |
| Local SGD for Near-Quadratic Problems: Improving Convergence under
  Unconstrained Noise Conditions | Andrey Sadchikov, Savelii Chezhegov, Aleksandr Beznosikov, Alexander Gasnikov |   Distributed optimization plays an important role in modern large-scale
machine learning and data processing systems by optimizing the utilization of
computational resources. One of the classical and popular approaches is Local
Stochastic Gradient Descent (Local SGD), characterized by multiple local
updates before averaging, which is particularly useful in distributed
environments to reduce communication bottlenecks and improve scalability. A
typical feature of this method is the dependence on the frequency of
communications. But in the case of a quadratic target function with homogeneous
data distribution over all devices, the influence of frequency of
communications vanishes. As a natural consequence, subsequent studies include
the assumption of a Lipschitz Hessian, as this indicates the similarity of the
optimized function to a quadratic one to some extent. However, in order to
extend the completeness of the Local SGD theory and unlock its potential, in
this paper we abandon the Lipschitz Hessian assumption by introducing a new
concept of $\textit{approximate quadraticity}$. This assumption gives a new
perspective on problems that have near quadratic properties. In addition,
existing theoretical analyses of Local SGD often assume bounded variance. We,
in turn, consider the unbounded noise condition, which allows us to broaden the
class of studied problems.
 | General | [Link](http://arxiv.org/abs/2409.10478v1) |
| SimInversion: A Simple Framework for Inversion-Based Text-to-Image
  Editing | Qi Qian, Haiyang Xu, Ming Yan, Juhua Hu |   Diffusion models demonstrate impressive image generation performance with
text guidance. Inspired by the learning process of diffusion, existing images
can be edited according to text by DDIM inversion. However, the vanilla DDIM
inversion is not optimized for classifier-free guidance and the accumulated
error will result in the undesired performance. While many algorithms are
developed to improve the framework of DDIM inversion for editing, in this work,
we investigate the approximation error in DDIM inversion and propose to
disentangle the guidance scale for the source and target branches to reduce the
error while keeping the original framework. Moreover, a better guidance scale
(i.e., 0.5) than default settings can be derived theoretically. Experiments on
PIE-Bench show that our proposal can improve the performance of DDIM inversion
dramatically without sacrificing efficiency.
 | General | [Link](http://arxiv.org/abs/2409.10476v1) |
| Leadership and Engagement Dynamics in Legislative Twitter Networks:
  Statistical Analysis and Modeling | Carolina Luque, Juan Sosa |   In this manuscript, we analyze the interaction network on Twitter among
members of the 117th U.S. Congress to assess the visibility of political
leaders and explore how systemic properties and node attributes influence the
formation of legislative connections. We employ descriptive social network
statistical methods, the exponential random graph model (ERGM), and the
stochastic block model (SBM) to evaluate the relative impact of network
systemic properties, as well as institutional and personal traits, on the
generation of online relationships among legislators. Our findings reveal that
legislative networks on social media platforms like Twitter tend to reinforce
the leadership of dominant political actors rather than diminishing their
influence. However, we identify that these leadership roles can manifest in
various forms. Additionally, we highlight that online connections within
legislative networks are influenced by both the systemic properties of the
network and institutional characteristics.
 | General | [Link](http://arxiv.org/abs/2409.10475v1) |
| MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion | Lehong Wu, Lilang Lin, Jiahang Zhang, Yiyang Ma, Jiaying Liu |   Self-supervised learning has proved effective for skeleton-based human action
understanding. However, previous works either rely on contrastive learning that
suffers false negative problems or are based on reconstruction that learns too
much unessential low-level clues, leading to limited representations for
downstream tasks. Recently, great advances have been made in generative
learning, which is naturally a challenging yet meaningful pretext task to model
the general underlying data distributions. However, the representation learning
capacity of generative models is under-explored, especially for the skeletons
with spacial sparsity and temporal redundancy. To this end, we propose Masked
Conditional Diffusion (MacDiff) as a unified framework for human skeleton
modeling. For the first time, we leverage diffusion models as effective
skeleton representation learners. Specifically, we train a diffusion decoder
conditioned on the representations extracted by a semantic encoder. Random
masking is applied to encoder inputs to introduce a information bottleneck and
remove redundancy of skeletons. Furthermore, we theoretically demonstrate that
our generative objective involves the contrastive learning objective which
aligns the masked and noisy views. Meanwhile, it also enforces the
representation to complement for the noisy view, leading to better
generalization performance. MacDiff achieves state-of-the-art performance on
representation learning benchmarks while maintaining the competence for
generative tasks. Moreover, we leverage the diffusion model for data
augmentation, significantly enhancing the fine-tuning performance in scenarios
with scarce labeled data. Our project is available at
https://lehongwu.github.io/ECCV24MacDiff/.
 | General | [Link](http://arxiv.org/abs/2409.10473v1) |
| Towards Semantic Versioning of Open Pre-trained Language Model Releases
  on Hugging Face | Adekunle Ajibode, Abdul Ali Bangash, Filipe Roseiro Cogo, Bram Adams, Ahmed E. Hassan |   The proliferation of open Pre-trained Language Models (PTLMs) on model
registry platforms like Hugging Face (HF) presents both opportunities and
challenges for companies building products around them. Similar to traditional
software dependencies, PTLMs continue to evolve after a release. However, the
current state of release practices of PTLMs on model registry platforms are
plagued by a variety of inconsistencies, such as ambiguous naming conventions
and inaccessible model training documentation. Given the knowledge gap on
current PTLM release practices, our empirical study uses a mixed-methods
approach to analyze the releases of 52,227 PTLMs on the most well-known model
registry, HF. Our results reveal 148 different naming practices for PTLM
releases, with 40.87% of changes to model weight files not represented in the
adopted name-based versioning practice or their documentation. In addition, we
identified that the 52,227 PTLMs are derived from only 299 different base
models (the modified original models used to create 52,227 PTLMs), with
Fine-tuning and Quantization being the most prevalent modification methods
applied to these base models. Significant gaps in release transparency, in
terms of training dataset specifications and model card availability, still
exist, highlighting the need for standardized documentation. While we
identified a model naming practice explicitly differentiating between major and
minor PTLM releases, we did not find any significant difference in the types of
changes that went into either type of releases, suggesting that major/minor
version numbers for PTLMs often are chosen arbitrarily. Our findings provide
valuable insights to improve PTLM release practices, nudging the field towards
more formal semantic versioning practices.
 | General | [Link](http://arxiv.org/abs/2409.10472v1) |
| Real-Time Whole-Body Control of Legged Robots with Model-Predictive Path
  Integral Control | Juan Alvarez-Padilla, John Z. Zhang, Sofia Kwok, John M. Dolan, Zachary Manchester |   This paper presents a system for enabling real-time synthesis of whole-body
locomotion and manipulation policies for real-world legged robots. Motivated by
recent advancements in robot simulation, we leverage the efficient
parallelization capabilities of the MuJoCo simulator to achieve fast sampling
over the robot state and action trajectories. Our results show surprisingly
effective real-world locomotion and manipulation capabilities with a very
simple control strategy. We demonstrate our approach on several hardware and
simulation experiments: robust locomotion over flat and uneven terrains,
climbing over a box whose height is comparable to the robot, and pushing a box
to a goal position. To our knowledge, this is the first successful deployment
of whole-body sampling-based MPC on real-world legged robot hardware.
Experiment videos and code can be found at: https://whole-body-mppi.github.io/
 | General | [Link](http://arxiv.org/abs/2409.10469v1) |
| Assortativity in sympatric speciation and species classification | Joao U. F. Lizarraga, Flavia M. D. Marquitti, Marcus A. M. de Aguiar |   We investigate the role of assortative mating in speciation using the
sympatric model of Derrida and Higgs. The model explores the idea that genetic
differences create incompatibilities between individuals, preventing mating if
the number of such differences is too large. Speciation, however, only happens
in this mating system if the number of genes is large. Here we show that
speciation with small genome sizes can occur if assortative mating is
introduced. In our model individuals are represented by three chromosomes: one
responsible for reproductive compatibility, one for coding the trait on which
assortativity will operate, and a neutral chromosome. Reproduction is possible
if individuals are genetically similar with respect to the first chromosome,
but among these compatible mating partners, the one with the most similar trait
coded by the second chromosome is selected. We show that this type of
assortativity facilitates speciation, which can happen with a small number of
genes in the first chromosome. Species, classified according to reproductive
isolation, dictated by the first chromosome, can display different traits
values, as measured by the second and the third chromosomes. Therefore, species
can also be identified based on similarity of the neutral trait, which works as
a proxy for reproductive isolation.
 | General | [Link](http://arxiv.org/abs/2409.10466v1) |
| A stabilized total pressure-formulation of the Biot's poroelasticity
  equations in frequency domain: numerical analysis and applications | Cristian Cárcamo, Alfonso Caiazzo, Felipe Galarce, Joaquín Mura |   This work focuses on the numerical solution of the dynamics of a poroelastic
material in the frequency domain. We provide a detailed stability analysis
based on the application of the Fredholm alternative in the continuous case,
considering a total pressure formulation of the Biot's equations. In the
discrete setting, we propose a stabilized equal order finite element method
complemented by an additional pressure stabilization to enhance the robustness
of the numerical scheme with respect to the fluid permeability. Utilizing the
Fredholm alternative, we extend the well-posedness results to the discrete
setting, obtaining theoretical optimal convergence for the case of linear
finite elements. We present different numerical experiments to validate the
proposed method. First, we consider model problems with known analytic
solutions in two and three dimensions. As next, we show that the method is
robust for a wide range of permeabilities, including the case of discontinuous
coefficients. Lastly, we show the application for the simulation of brain
elastography on a realistic brain geometry obtained from medical imaging.
 | General | [Link](http://arxiv.org/abs/2409.10465v1) |
| New Direct Sum Tests | Alek Westover, Edward Yu, Kai Zheng |   A function $f:[n]^{d} \to \mathbb{F}_2$ is a \defn{direct sum} if there are
functions $L_i:[n]\to \mathbb{F}_2$ such that ${f(x) = \sum_{i}L_i(x_i)}$. In
this work we give multiple results related to the property testing of direct
sums.
  Our first result concerns a test proposed by Dinur and Golubev in 2019. We
call their test the Diamond test and show that it is indeed a direct sum
tester. More specifically, we show that if a function $f$ is $\epsilon$-far
from being a direct sum function, then the Diamond test rejects $f$ with
probability at least $\Omega_{n,\epsilon}(1)$. Even in the case of $n = 2$, the
Diamond test is, to the best of our knowledge, novel and yields a new tester
for the classic property of affinity.
  Apart from the Diamond test, we also analyze a broad family of direct sum
tests, which at a high level, run an arbitrary affinity test on the restriction
of $f$ to a random hypercube inside of $[n]^d$. This family of tests includes
the direct sum test analyzed in \cite{di19}, but does not include the Diamond
test. As an application of our result, we obtain a direct sum test which works
in the online adversary model of \cite{KRV}.
  Finally, we also discuss a Fourier analytic interpretation of the diamond
tester in the $n=2$ case, as well as prove local correction results for direct
sum as conjectured by Dinur and Golubev.
 | General | [Link](http://arxiv.org/abs/2409.10464v1) |
| Kolmogorov-Arnold Networks in Low-Data Regimes: A Comparative Study with
  Multilayer Perceptrons | Farhad Pourkamali-Anaraki |   Multilayer Perceptrons (MLPs) have long been a cornerstone in deep learning,
known for their capacity to model complex relationships. Recently,
Kolmogorov-Arnold Networks (KANs) have emerged as a compelling alternative,
utilizing highly flexible learnable activation functions directly on network
edges, a departure from the neuron-centric approach of MLPs. However, KANs
significantly increase the number of learnable parameters, raising concerns
about their effectiveness in data-scarce environments. This paper presents a
comprehensive comparative study of MLPs and KANs from both algorithmic and
experimental perspectives, with a focus on low-data regimes. We introduce an
effective technique for designing MLPs with unique, parameterized activation
functions for each neuron, enabling a more balanced comparison with KANs. Using
empirical evaluations on simulated data and two real-world data sets from
medicine and engineering, we explore the trade-offs between model complexity
and accuracy, with particular attention to the role of network depth. Our
findings show that MLPs with individualized activation functions achieve
significantly higher predictive accuracy with only a modest increase in
parameters, especially when the sample size is limited to around one hundred.
For example, in a three-class classification problem within additive
manufacturing, MLPs achieve a median accuracy of 0.91, significantly
outperforming KANs, which only reach a median accuracy of 0.53 with default
hyperparameters. These results offer valuable insights into the impact of
activation function selection in neural networks.
 | General | [Link](http://arxiv.org/abs/2409.10463v1) |
| Effects of synapse location, delay and background stochastic activity on
  synchronising hippocampal CA1 neurons | Alessandro Fiasconaro, Michele Migliore |   We study the synchronisation of neurons in a realistic model under the
Hodgkin-Huxley dynamics. To focus on the role of the different locations of the
excitatory synapses, we use two identical neurons where the set of input
signals is grouped at two different distances from the soma. The system is
intended to represent a CA1 hippocampal neuron in which the synapses arriving
from the CA3 neurons of the trisynaptic pathway appear to be localised in the
apical dendritic region and are, in principle, either proximal or distal to the
soma. Synchronisation is studied using a specifically defined spiking
correlation function as a function of various parameters such as the distance
from the soma of one of the synaptic groups, the inhibition weight and the
associated activation delay. We found that the neurons' spiking activity
depends nonmonotonically on the relative dendritic location of the synapses and
their inhibitory weight, whereas the synchronisation measure always decreases
with inhibition, and strongly depends on its activation time delay. The
background activity on the somas results essentially independent on the
fluctuation intensity and strongly support the importance of the balance
between inhibition and excitation for neuronal synchronization.
 | General | [Link](http://arxiv.org/abs/2409.10460v1) |
| Efficiently Crowdsourcing Visual Importance with Punch-Hole Annotation | Minsuk Chang, Soohyun Lee, Aeri Cho, Hyeon Jeon, Seokhyeon Park, Cindy Xiong Bearfield, Jinwook Seo |   We introduce a novel crowdsourcing method for identifying important areas in
graphical images through punch-hole labeling. Traditional methods, such as gaze
trackers and mouse-based annotations, which generate continuous data, can be
impractical in crowdsourcing scenarios. They require many participants, and the
outcome data can be noisy. In contrast, our method first segments the graphical
image with a grid and drops a portion of the patches (punch holes). Then, we
iteratively ask the labeler to validate each annotation with holes, narrowing
down the annotation only having the most important area. This approach aims to
reduce annotation noise in crowdsourcing by standardizing the annotations while
enhancing labeling efficiency and reliability. Preliminary findings from
fundamental charts demonstrate that punch-hole labeling can effectively
pinpoint critical regions. This also highlights its potential for broader
application in visualization research, particularly in studying large-scale
users' graphical perception. Our future work aims to enhance the algorithm to
achieve faster labeling speed and prove its utility through large-scale
experiments.
 | General | [Link](http://arxiv.org/abs/2409.10459v1) |
| Full distribution of local observables in an exactly solvable current
  carrying steady states of a driven XXZ chain | Sandipan Manna, G J Sreejith |   Current carrying steady states of interacting spins chains exhibit rich
structures generated through an interplay of current induced correlations and
energetic constraints from the Hamiltonian. The XXZ spin chain when coupled to
maximally polarizing Lindblad terms admits an exact solution in a matrix
product state (MPS) form. We use this exact solution to study the correlations
and distributions of simple local spin observables in the non equilibrium
steady state (NESS). We present exact expressions for spin correlators, entropy
per site, cumulant generating functions for distributions of local observables
in the XX limit (Ising anisotropy $\Delta=0$). Further, we use the exact MPS
solution in systems with $\Delta>0$, to numerically exactly calculate the
entropy, correlations, as well as distributions of spin observables in blocks
as large as $n\sim 200$ sites allowing an estimation of the rate functions. The
$z$ magnetization distribution is consistant with short range spin correlations
in the $z$ direction while the $x$-magnetization shows a double peak structure
at larger $\Delta$ suggesting short range ferromagnetic ordering. We find that
the distribution of $z$-magnetization sharpens for parameters where the current
is maximized.
 | General | [Link](http://arxiv.org/abs/2409.10458v1) |
| Mean Residual Life Ageing Intensity Function | Ashutosh Singh, Ishapathik Das, Asok Kumar Nanda, Sumen Sen |   The ageing intensity function is a powerful analytical tool that provides
valuable insights into the ageing process across diverse domains such as
reliability engineering, actuarial science, and healthcare. Its applications
continue to expand as researchers delve deeper into understanding the complex
dynamics of ageing and its implications for society. One common approach to
defining the ageing intensity function is through the hazard rate or failure
rate function, extensively explored in scholarly literature. Equally
significant to the hazard rate function is the mean residual life function,
which plays a crucial role in analyzing the ageing patterns exhibited by units
or components. This article introduces the mean residual life ageing intensity
(MRLAI) function to delve into component ageing behaviours across various
distributions. Additionally, we scrutinize the closure properties of the MRLAI
function across different reliability operations. Furthermore, a new order
termed the mean residual life ageing intensity order is defined to analyze the
ageing behaviour of a system, and the closure property of this order under
various reliability operations is discussed.
 | General | [Link](http://arxiv.org/abs/2409.10456v1) |
| Quantile Fourier regressions for decision making under uncertainty | Arash Khojaste, Geoffrey Pritchard, Golbon Zakeri |   Weconsider Markov decision processes arising from a Markov model of an
underlying natural phenomenon. Such phenomena are usually periodic (e.g.
annual) in time, and so the Markov processes modelling them must be
time-inhomogeneous, with cyclostationary rather than stationary behaviour. We
describe a technique for constructing such processes that allows for periodic
variations both in the values taken by the process and in the serial dependence
structure. We include two illustrative numerical examples: a hydropower
scheduling problem and a model of offshore wind power integration.
 | General | [Link](http://arxiv.org/abs/2409.10455v1) |
| The Debiased Near-Earth Object Population from ATLAS Telescopes | Rogerio Deienno, Larry Denneau, David Nesvorný, David Vokrouhlický, William F. Bottke, Robert Jedicke, Shantanu Naidu, Steven R. Chesley, Davide Farnocchia, Paul W. Chodas |   This work is dedicated to debias the Near-Earth Objects (NEO) population
based on observations from the Asteroid Terrestrial-impact Last Alert System
(ATLAS) telescopes. We have applied similar methods used to develop the
recently released NEO model generator (NEOMOD), once debiasing the NEO
population using data from Catalina Sky Survey (CSS) G96 telescope. ATLAS is
composed of four different telescopes. We first analyzed observational data
from each of all four telescopes separately and later combined them. Our
results highlight main differences between CSS and ATLAS, e.g., sky coverage
and survey power at debiasing the NEO population. ATLAS has a much larger sky
coverage than CSS, allowing it to find bright NEOs that would be constantly
"hiding" from CSS. Consequently, ATLAS is more powerful than CSS at debiasing
the NEO population for H $\lesssim$ 19. With its intrinsically greater
sensitivity and emphasis on observing near opposition, CSS excels in the
debiasing of smaller objects. ATLAS, as an all sky survey designed to find
imminent hazardous objects, necessarily spends a significant fraction of time
looking at places on the sky where objects do not appear, reducing its power
for debiasing the population of small objects. We estimate a NEO population
completeness of $\approx$ 88%$^{+3\%}_{-2\%}$ for H $<$ 17.75 and $\approx$
36%$^{+1\%}_{-1\%}$ for H $<$ 22.25. Those numbers are similar to previous
estimates (within error bars for H $<$ 17.75) from CSS, yet, around 3% and 8%
smaller at their face values, respectively. We also confirm previous finding
that the $\nu_6$ secular resonance is the main source of small and faint NEOs
at H = 28, whereas the 3:1 mean motion resonance with Jupiter dominates for
larger and brighter NEOs at H = 15.
 | General | [Link](http://arxiv.org/abs/2409.10453v1) |
| Signed Graph Autoencoder for Explainable and Polarization-Aware Network
  Embeddings | Nikolaos Nakis, Chrysoula Kosma, Giannis Nikolentzos, Michalis Chatzianastasis, Iakovos Evdaimon, Michalis Vazirgiannis |   Autoencoders based on Graph Neural Networks (GNNs) have garnered significant
attention in recent years for their ability to extract informative latent
representations, characterizing the structure of complex topologies, such as
graphs. Despite the prevalence of Graph Autoencoders, there has been limited
focus on developing and evaluating explainable neural-based graph generative
models specifically designed for signed networks. To address this gap, we
propose the Signed Graph Archetypal Autoencoder (SGAAE) framework. SGAAE
extracts node-level representations that express node memberships over distinct
extreme profiles, referred to as archetypes, within the network. This is
achieved by projecting the graph onto a learned polytope, which governs its
polarization. The framework employs a recently proposed likelihood for
analyzing signed networks based on the Skellam distribution, combined with
relational archetypal analysis and GNNs. Our experimental evaluation
demonstrates the SGAAEs' capability to successfully infer node memberships over
the different underlying latent structures while extracting competing
communities formed through the participation of the opposing views in the
network. Additionally, we introduce the 2-level network polarization problem
and show how SGAAE is able to characterize such a setting. The proposed model
achieves high performance in different tasks of signed link prediction across
four real-world datasets, outperforming several baseline models.
 | General | [Link](http://arxiv.org/abs/2409.10452v1) |
| Why you should also use OLS estimation of tail exponents | Thiago Trafane Oliveira Santos, Daniel Oliveira Cajueiro |   Even though practitioners often estimate Pareto exponents running OLS
rank-size regressions, the usual recommendation is to use the Hill MLE with a
small-sample correction instead, due to its unbiasedness and efficiency. In
this paper, we advocate that you should also apply OLS in empirical
applications. On the one hand, we demonstrate that, with a small-sample
correction, the OLS estimator is also unbiased. On the other hand, we show that
the MLE assigns significantly greater weight to smaller observations. This
suggests that the OLS estimator may outperform the MLE in cases where the
distribution is (i) strictly Pareto but only in the upper tail or (ii)
regularly varying rather than strictly Pareto. We substantiate our theoretical
findings with Monte Carlo simulations and real-world applications,
demonstrating the practical relevance of the OLS method in estimating tail
exponents.
 | General | [Link](http://arxiv.org/abs/2409.10448v1) |
| Hydrodynamic hovering of swimming bacteria above surfaces | Pyae Hein Htet, Debasish Das, Eric Lauga |   Flagellated bacteria are hydrodynamically attracted to rigid walls, yet past
work shows a 'hovering' state where they swim stably at a finite height above
surfaces. We use numerics and theory to reveal the physical origin of hovering.
Simulations first show that hovering requires an elongated cell body and
results from a tilt away from the wall. Theoretical models then identify two
essential asymmetries: the response of width-asymmetric cells to active flows
created by length-asymmetric cells. A minimal model reconciles near and
far-field hydrodynamics, capturing all key features of hovering.
 | General | [Link](http://arxiv.org/abs/2409.10447v1) |
| KoroT-3E: A Personalized Musical Mnemonics Tool for Enhancing Memory
  Retention of Complex Computer Science Concepts | Xiangzhe Yuan, Jiajun Wang, Siying Hu, Andrew Cheung, Zhicong Lu |   As the demand for computer science (CS) skills grows, mastering foundational
concepts is crucial yet challenging for novice learners. To address this
challenge, we present KoroT-3E, an AI-based system that creates personalized
musical mnemonics to enhance both memory retention and understanding of
concepts in CS. KoroT-3E enables users to transform complex concepts into
memorable lyrics and compose melodies that suit their musical preferences. We
conducted semi-structured interviews (n=12) to investigate why novice learners
find it challenging to memorize and understand CS concepts. The findings,
combined with constructivist learning theory, established our initial design,
which was then refined following consultations with CS education experts. An
empirical experiment(n=36) showed that those using KoroT-3E (n=18)
significantly outperformed the control group (n=18), with improved memory
efficiency, increased motivation, and a positive learning experience. These
findings demonstrate the effectiveness of integrating multimodal generative AI
into CS education to create personalized and interactive learning experiences.
 | General | [Link](http://arxiv.org/abs/2409.10446v1) |
| Deep-Wide Learning Assistance for Insect Pest Classification | Toan Nguyen, Huy Nguyen, Huy Ung, Hieu Ung, Binh Nguyen |   Accurate insect pest recognition plays a critical role in agriculture. It is
a challenging problem due to the intricate characteristics of insects. In this
paper, we present DeWi, novel learning assistance for insect pest
classification. With a one-stage and alternating training strategy, DeWi
simultaneously improves several Convolutional Neural Networks in two
perspectives: discrimination (by optimizing a triplet margin loss in a
supervised training manner) and generalization (via data augmentation). From
that, DeWi can learn discriminative and in-depth features of insect pests
(deep) yet still generalize well to a large number of insect categories (wide).
Experimental results show that DeWi achieves the highest performances on two
insect pest classification benchmarks (76.44\% accuracy on the IP102 dataset
and 99.79\% accuracy on the D0 dataset, respectively). In addition, extensive
evaluations and ablation studies are conducted to thoroughly investigate our
DeWi and demonstrate its superiority. Our source code is available at
https://github.com/toannguyen1904/DeWi.
 | General | [Link](http://arxiv.org/abs/2409.10445v1) |
| LLM as BT-Planner: Leveraging LLMs for Behavior Tree Generation in Robot
  Task Planning | Jicong Ao, Fan Wu, Yansong Wu, Abdalla Swikir, Sami Haddadin |   Robotic assembly tasks are open challenges due to the long task horizon and
complex part relations. Behavior trees (BTs) are increasingly used in robot
task planning for their modularity and flexibility, but manually designing them
can be effort-intensive. Large language models (LLMs) have recently been
applied in robotic task planning for generating action sequences, but their
ability to generate BTs has not been fully investigated. To this end, We
propose LLM as BT-planner, a novel framework to leverage LLMs for BT generation
in robotic assembly task planning and execution. Four in-context learning
methods are introduced to utilize the natural language processing and inference
capabilities of LLMs to produce task plans in BT format, reducing manual effort
and ensuring robustness and comprehensibility. We also evaluate the performance
of fine-tuned, fewer-parameter LLMs on the same tasks. Experiments in simulated
and real-world settings show that our framework enhances LLMs' performance in
BT generation, improving success rates in BT generation through in-context
learning and supervised fine-tuning.
 | General | [Link](http://arxiv.org/abs/2409.10444v1) |
| The James Webb Space Telescope Absolute Flux Calibration. II.
  Mid-Infrared Instrument Imaging and Coronagraphy | Karl D. Gordon, G. C. Sloan, Macarena Garcia Marin, Mattia Libralato, George Rieke, Jonathan A. Aguilar, Ralph Bohlin, Misty Cracraft, Marjorie Decleir, Andras Gaspar, David R. Law, Alberto Noriega-Crespo, Michael Regan |   The absolute flux calibration of the Mid-Infrared Instrument Imaging and
Coronagraphy is based on observations of multiple stars taken during the first
2.5 years of JWST operations. The observations were designed to ensure that the
flux calibration is valid for a range of flux densities, different subarrays,
and different types of stars. The flux calibration was measured by combining
observed aperture photometry corrected to infinite aperture with predictions
based on previous observations and models of stellar atmospheres. A subset of
these observations were combined with model point-spread-functions to measure
the corrections to infinite aperture. Variations in the calibration factor with
time, flux density, background level, type of star, subarray, integration time,
rate, and well depth were investigated, and the only significant variations
were with time and subarray. Observations of the same star taken approximately
every month revealed a modest time-dependent response loss seen mainly at the
longest wavelengths. This loss is well characterized by a decaying exponential
with a time constant of ~200 days. After correcting for the response loss, the
band-dependent scatter around the corrected average (aka repeatability) was
found to range from 0.1 to 1.2%. Signals in observations taken with different
subarrays can be lower by up to 3.4% compared to FULL frame. After correcting
for the time and subarray dependencies, the scatter in the calibration factors
measured for individual stars ranges from 1 to 4% depending on the band. The
formal uncertainties on the flux calibration averaged for all observations are
0.3 to 1.0%, with longer-wavelength bands generally having larger
uncertainties.
 | General | [Link](http://arxiv.org/abs/2409.10443v1) |
| CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using
  a Single Camera | Jingpei Lu, Zekai Liang, Tristin Xie, Florian Ritcher, Shan Lin, Sainan Liu, Michael C. Yip |   Camera-to-robot calibration is crucial for vision-based robot control and
requires effort to make it accurate. Recent advancements in markerless pose
estimation methods have eliminated the need for time-consuming physical setups
for camera-to-robot calibration. While the existing markerless pose estimation
methods have demonstrated impressive accuracy without the need for cumbersome
setups, they rely on the assumption that all the robot joints are visible
within the camera's field of view. However, in practice, robots usually move in
and out of view, and some portion of the robot may stay out-of-frame during the
whole manipulation task due to real-world constraints, leading to a lack of
sufficient visual features and subsequent failure of these approaches. To
address this challenge and enhance the applicability to vision-based robot
control, we propose a novel framework capable of estimating the robot pose with
partially visible robot manipulators. Our approach leverages the
Vision-Language Models for fine-grained robot components detection, and
integrates it into a keypoint-based pose estimation network, which enables more
robust performance in varied operational conditions. The framework is evaluated
on both public robot datasets and self-collected partial-view datasets to
demonstrate our robustness and generalizability. As a result, this method is
effective for robot pose estimation in a wider range of real-world manipulation
scenarios.
 | General | [Link](http://arxiv.org/abs/2409.10441v1) |
| Successive topological phase transitions in two distinct spin-flop
  phases on the honeycomb lattice | Xudong Li, Jize Zhao, Jinbin Li, Qiang Luo |   The Kitaev magnets with bond-dependent interactions have garnered
considerable attention in recent years for their ability to harbor exotic
phases and nontrivial excitations. The topological magnons, which are indicated
by nonzero Chern number and thermal Hall conductivity, are proposed to
partially explain thermal Hall measurements in real materials. Hitherto,
topological magnons have been extensively explored when the magnetic field is
normal to the honeycomb plane, but their topological characteristics are less
studied in the presence of in-plane magnetic field. Here, we study two distinct
in-plane field induced spin-flop phases in the $\Gamma$-$\Gamma'$ model, both
of which are off-diagonal couplings that have intimate relation to the Kitaev
interaction. The two spin-flop phases are distinguished by their out-of-plane
spin components which can be either antiparallel or parallel, thus dubbing
antiferromagnetic (AFM) or ferromagnetic (FM) spin-flop phases, respectively.
We map out topological phase diagrams for both phases, revealing a rich pattern
of the Chern number over exchange parameters and magnetic field. We
analytically calculate the boundaries of topological phase transitions when the
magnetic field is along the $a$ and $b$ directions. We find that the thermal
Hall conductivity and its derivative display contrasting behaviors when
crossing different topological phase transitions. The striking difference of
the two phases lies in that when the magnetic field is along the $b$ direction,
topological magnons are totally absent in the AFM spin-flop phase, while they
can survive in the FM analogue in certain parameter regions.
 | General | [Link](http://arxiv.org/abs/2409.10439v1) |
| A functorial approach to $n$-abelian categories | Vitor Gulisz |   We develop a functorial approach to the study of $n$-abelian categories by
reformulating their axioms in terms of their categories of finitely presented
functors. Such an approach allows the use of classical homological algebra and
representation theory techniques to understand higher homological algebra. As
an application, we present two possible generalizations of the axioms "every
monomorphism is a kernel" and "every epimorphism is a cokernel" of an abelian
category to $n$-abelian categories. We also specialize our results to modules
over rings, thereby describing when the category of finitely generated
projective modules over a ring is $n$-abelian. Moreover, we establish a
correspondence for $n$-abelian categories with additive generators, which
extends the higher Auslander correspondence.
 | General | [Link](http://arxiv.org/abs/2409.10438v1) |
| Color symmetry breaking in the Potts spin glass | Jean-Christophe Mourrat |   The Potts spin glass is an analogue of the Sherrington-Kirkpatrick model in
which each spin can take one of $\kappa$ possible values, which we interpret as
colors. It was suggested in arXiv:2310.06745 that the order parameter for this
model is always invariant with respect to permutations of the colors. We show
here that this is false whenever $\kappa \ge 58$.
 | General | [Link](http://arxiv.org/abs/2409.10437v1) |
| Infector characteristics exposed by spatial analysis of SARS-CoV-2
  sequence and demographic data analysed at fine geographical scales | Anna Gamża, Samantha Lycett, Will Harvey, Joseph Hughes, Sema Nickbakhsh, David L Robertson, Alison Smith Palmer, Anthony Wood, Rowland Kao |   Characterising drivers of SARS-CoV-2 circulation is crucial for understanding
COVID-19 because of the severity of control measures adopted during the
pandemic. Whole genome sequence data augmented with demographic metadata
provides the best opportunity to do this. We use Random Forest Decision Tree
models to analyse a combination of over 4000 SARS-CoV2 sequences from a densely
sampled, mixed urban and rural population (Tayside) in Scotland in the period
from August 2020 to July 2021, with fine scale geographical and
socio-demographic metadata. Comparing periods in versus out of "lockdown"
restrictions, we show using genetic distance relationships that individuals
from more deprived areas are more likely to get infected during lockdown but
less likely to spread the infection further. As disadvantaged communities were
the most affected by both COVID-19 and its restrictions, our finding has
important implications for informing future approaches to control future
pandemics driven by similar respiratory infections.
 | General | [Link](http://arxiv.org/abs/2409.10436v1) |
| A study of pure multi-strange hadrons production in Pb+Pb collisions at
  LHC energies using HYDJET++ model | Gauri Devi, B. K. Singh |   For the present work, we have used the HYDJET++ model to explore the
production of pure multi-strange hadrons in Pb+Pb collisions at
$\sqrt{s_{NN}}$= 2.76 TeV and $\sqrt{s_{NN}}$= 5.02 TeV collision energies,
respectively. We have simulated the $p_{T}$-spectra and elliptic flow ($v_{2}$)
of $\phi$-meson and $\Omega$-baryons comparing our results with ALICE
experimental data and several phenomenological models (HIJING/$B\overline{B}$,
VISHNU, EPOS, AMPT, and Krakow) across various centrality intervals.
Additionally, we have calculated the nuclear modification factors ($R_{AA}$ and
$R_{CP}$), which provide a perception of jet quenching phenomena. Hence, our
findings enable the study of the energy and system dependence of $\phi$ and
$\Omega$ hadrons production over a wide range of ultra-relativistic collision
energies. We also present the particle ratios ($\Omega/\phi$,
$\overline{\Omega}^{+}/\Omega^{-}$, $\Omega/\pi$, and $\phi/\pi$), offering
insights on the strangeness enhancement and chemical properties of the medium
at both LHC collision energies.
 | General | [Link](http://arxiv.org/abs/2409.10435v1) |
| Magnetic metamaterials by ion-implantation | Christina Vantaraki, Petter Ström, Tuan T. Tran, Matías P. Grassi, Giovanni Fevola, Michael Foerster, Jerzy T. Sadowski, Daniel Primetzhofer, Vassilios Kapaklis |   We present a method for the additive fabrication of planar magnetic
nanoarrays with minimal surface roughness. Synthesis is accomplished by
combining electron-beam lithography, used to generate nanometric patterned
masks, with ion implantation in thin films. By implanting $^{56}$Fe$^{+}$ ions,
we are able to introduce magnetic functionality in a controlled manner into
continuous Pd thin films, achieving 3D spatial resolution down to a few tens of
nanometers. Our results demonstrate the successful application of this
technique in fabricating square artificial spin ice lattices, which exhibit
well-defined magnetization textures and interactions among the patterned
magnetic elements.
 | General | [Link](http://arxiv.org/abs/2409.10433v1) |
| Structure-preserving learning for multi-symplectic PDEs | Süleyman Yıldız, Pawan Goyal, Peter Benner |   This paper presents an energy-preserving machine learning method for
inferring reduced-order models (ROMs) by exploiting the multi-symplectic form
of partial differential equations (PDEs). The vast majority of
energy-preserving reduced-order methods use symplectic Galerkin projection to
construct reduced-order Hamiltonian models by projecting the full models onto a
symplectic subspace. However, symplectic projection requires the existence of
fully discrete operators, and in many cases, such as black-box PDE solvers,
these operators are inaccessible. In this work, we propose an energy-preserving
machine learning method that can infer the dynamics of the given PDE using data
only, so that the proposed framework does not depend on the fully discrete
operators. In this context, the proposed method is non-intrusive. The proposed
method is grey box in the sense that it requires only some basic knowledge of
the multi-symplectic model at the partial differential equation level. We prove
that the proposed method satisfies spatially discrete local energy conservation
and preserves the multi-symplectic conservation laws. We test our method on the
linear wave equation, the Korteweg-de Vries equation, and the
Zakharov-Kuznetsov equation. We test the generalization of our learned models
by testing them far outside the training time interval.
 | General | [Link](http://arxiv.org/abs/2409.10432v1) |
| Hippocampal synchronization in a realistic CA1 neuron model | Alessandro Fiasconaro, Michele Migliore |   This work delves into studying the synchronization in two realistic neuron
models using Hodgkin-Huxley dynamics. Unlike simplistic point-like models,
excitatory synapses are here randomly distributed along the dendrites,
introducing strong stochastic contributions into their signal propagation. To
focus on the role of different excitatory positions, we use two copies of the
same neuron whose synapses are located at different distances from the soma and
are exposed to identical Poissonian distributed current pulses. The
synchronization is investigated through a specifically defined spiking
correlation function, and its behavior is analyzed as a function of several
parameters: inhibition weight, distance from the soma of one synaptic group,
excitatory inactivation delay, and weight of the excitatory synapses.
 | General | [Link](http://arxiv.org/abs/2409.10431v1) |
| Meta-Whisper: Speech-Based Meta-ICL for ASR on Low-Resource Languages | Ming-Hao Hsu, Kuan Po Huang, Hung-yi Lee |   This paper presents Meta-Whisper, a novel approach to improve automatic
speech recognition (ASR) for low-resource languages using the Whisper model. By
leveraging Meta In-Context Learning (Meta-ICL) and a k-Nearest Neighbors (KNN)
algorithm for sample selection, Meta-Whisper enhances Whisper's ability to
recognize speech in unfamiliar languages without extensive fine-tuning.
Experiments on the ML-SUPERB dataset show that Meta-Whisper significantly
reduces the Character Error Rate (CER) for low-resource languages compared to
the original Whisper model. This method offers a promising solution for
developing more adaptable multilingual ASR systems, particularly for languages
with limited resources.
 | General | [Link](http://arxiv.org/abs/2409.10429v1) |
| User-Authenticated Device-Independent Quantum Secure Direct
  Communication Protocol | Nayana Das, Saikat Basu, Goutam Paul, Vijay S. Rao |   Device-Independent Quantum Secure Direct Communication (DI-QSDC) enhances
quantum cryptography by enabling secure message transmission without relying on
the trustworthiness of the devices involved. This approach mitigates risks
associated with compromised or untrusted devices, common in traditional quantum
communication.
  In this paper, we propose the first of its kind DI-QSDC protocol with user
identity authentication. This ensures the authenticity of both the sender and
receiver prior to message exchange. We then discuss the security of the
proposed protocol against common attacks, demonstrating that no eavesdropper
gains any information from either the quantum or the classical channel. Next,
we implement the protocol on IBM's quantum hardware and evaluate its
performance in a realistic noisy environment. Additionally, by simulating
common attack models, we showcase that the protocol is secure against any
eavesdropper in the channel. These findings highlight the protocol's robust
security and practical feasibility for real-world secure quantum communication.
 | General | [Link](http://arxiv.org/abs/2409.10427v1) |
| Role of kinematic constraints in the time reversal symmetry breaking of
  a model active matter | Soumen Das, Shankar Ghosh, Tridib Sadhu, Juliane U Klamser |   Active-matter systems are inherently out-of-equilibrium and perform
mechanical work by utilizing their internal energy sources. Breakdown of
time-reversal symmetry (BTRS) is a hallmark of such dissipative non-equilibrium
dynamics. We introduce a robust, experimentally accessible, noninvasive,
quantitative measure of BTRS in terms of the Kullback-Leibler divergence in
collision events, demonstrated in our novel artificial active matter, comprised
of battery-powered spherical rolling robots whose energetics in different modes
of motion can be measured with high precision. Our dimensionless measure
characterizes how dissipation and internal energetics are influenced by
kinematic constraints from interactions with the environment. We propose this
measure of BTRS as an empirical estimate of the distance from equilibrium. An
energetic insight into this departure of active matter from equilibrium comes
from our demonstration of a non-trivial fluctuation symmetry, which reveals a
potentially universal thermodynamic characteristic of active energetics. As a
many-body consequence of BTRS in our experimental active system, we demonstrate
the emergence of activity-induced herding, which has no equilibrium analogue.
 | General | [Link](http://arxiv.org/abs/2409.10425v1) |
| Unveiling V Modes: Enhancing CMB Sensitivity to BSM Physics with a
  Non-Ideal Half-Wave Plate | N. Raffuzzi, M. Lembo, S. Giardiello, M. Gerbino, M. Lattanzi, P. Natoli, L. Pagano |   V-mode polarization of the cosmic microwave background is expected to be
vanishingly small in the $\Lambda$CDM model and, hence, usually ignored.
Nonetheless, several astrophysical effects, as well as beyond standard model
physics could produce it at a detectable level. A realistic half-wave plate -
an optical element commonly used in CMB experiments to modulate the polarized
signal - can provide sensitivity to V modes without significantly spoiling that
to linear polarization. We assess this sensitivity for some new-generation CMB
experiments, such as the LiteBIRD satellite, the ground-based Simons
Observatory and a CMB-S4-like experiment. We forecast the efficiency of these
experiments to constrain the phenomenology of certain classes of BSM models
inducing mixing of linear polarization states and generation of V modes in the
CMB. We find that new-generation experiments can improve current limits by
1-to-3 orders of magnitude, depending on the data combination. The inclusion of
V-mode information dramatically boosts the sensitivity to these BSM models.
 | General | [Link](http://arxiv.org/abs/2409.10424v1) |
| Initial data for a deformed isolated horizon | Aleš Flandera, David Kofroň, Tomáš Ledvinka |   Within the isolated horizon formalism, we investigate a static axisymmetric
space-time of a black hole influenced by matter in its neighborhood. To
illustrate the role of ingredients and assumptions in this formalism, we first
show how, in spherical symmetry, the field equations and gauge conditions imply
the isolated horizon initial data leading to the Schwarzschild space-time.
Then, we construct the initial data for a static axisymmetric isolated horizon
representing a deformed black hole. The space-time description in the
Bondi-like coordinates is then found as a series expansion in the vicinity of
the horizon. To graphically illustrate this construction, we also find a
numerical solution for a black hole deformed by a particular analytic model of
a thin accretion disk. We also discuss how an accretion disk affects the
analytical properties of the horizon geometry.
 | General | [Link](http://arxiv.org/abs/2409.10423v1) |
| Learning Semi-Supervised Medical Image Segmentation from Spatial
  Registration | Qianying Liu, Paul Henderson, Xiao Gu, Hang Dai, Fani Deligianni |   Semi-supervised medical image segmentation has shown promise in training
models with limited labeled data and abundant unlabeled data. However,
state-of-the-art methods ignore a potentially valuable source of unsupervised
semantic information -- spatial registration transforms between image volumes.
To address this, we propose CCT-R, a contrastive cross-teaching framework
incorporating registration information. To leverage the semantic information
available in registrations between volume pairs, CCT-R incorporates two
proposed modules: Registration Supervision Loss (RSL) and Registration-Enhanced
Positive Sampling (REPS). The RSL leverages segmentation knowledge derived from
transforms between labeled and unlabeled volume pairs, providing an additional
source of pseudo-labels. REPS enhances contrastive learning by identifying
anatomically-corresponding positives across volumes using registration
transforms. Experimental results on two challenging medical segmentation
benchmarks demonstrate the effectiveness and superiority of CCT-R across
various semi-supervised settings, with as few as one labeled case. Our code is
available at
https://github.com/kathyliu579/ContrastiveCross-teachingWithRegistration.
 | General | [Link](http://arxiv.org/abs/2409.10422v1) |
| Multidimensional Deconvolution with Profiling | Huanbiao Zhu, Krish Desai, Mikael Kuusela, Vinicius Mikuni, Benjamin Nachman, Larry Wasserman |   In many experimental contexts, it is necessary to statistically remove the
impact of instrumental effects in order to physically interpret measurements.
This task has been extensively studied in particle physics, where the
deconvolution task is called unfolding. A number of recent methods have shown
how to perform high-dimensional, unbinned unfolding using machine learning.
However, one of the assumptions in all of these methods is that the detector
response is accurately modeled in the Monte Carlo simulation. In practice, the
detector response depends on a number of nuisance parameters that can be
constrained with data. We propose a new algorithm called Profile OmniFold
(POF), which works in a similar iterative manner as the OmniFold (OF) algorithm
while being able to simultaneously profile the nuisance parameters. We
illustrate the method with a Gaussian example as a proof of concept
highlighting its promising capabilities.
 | General | [Link](http://arxiv.org/abs/2409.10421v1) |
| HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping
  Using Vision-Language Models | Vineet Bhat, Prashanth Krishnamurthy, Ramesh Karri, Farshad Khorrami |   Robots interacting with humans through natural language can unlock numerous
applications such as Referring Grasp Synthesis (RGS). Given a text query, RGS
determines a stable grasp pose to manipulate the referred object in the robot's
workspace. RGS comprises two steps: visual grounding and grasp pose estimation.
Recent studies leverage powerful Vision-Language Models (VLMs) for visually
grounding free-flowing natural language in real-world robotic execution.
However, comparisons in complex, cluttered environments with multiple instances
of the same object are lacking. This paper introduces HiFi-CS, featuring
hierarchical application of Featurewise Linear Modulation (FiLM) to fuse image
and text embeddings, enhancing visual grounding for complex attribute rich text
queries encountered in robotic grasping. Visual grounding associates an object
in 2D/3D space with natural language input and is studied in two scenarios:
Closed and Open Vocabulary. HiFi-CS features a lightweight decoder combined
with a frozen VLM and outperforms competitive baselines in closed vocabulary
settings while being 100x smaller in size. Our model can effectively guide
open-set object detectors like GroundedSAM to enhance open-vocabulary
performance. We validate our approach through real-world RGS experiments using
a 7-DOF robotic arm, achieving 90.33\% visual grounding accuracy in 15 tabletop
scenes. We include our codebase in the supplementary material.
 | General | [Link](http://arxiv.org/abs/2409.10419v1) |
| X-ray reverberation as an explanation for UV/optical variability in
  nearby Seyferts | M. Papoutsis, I. E. Papadakis, C. Panagiotou, M. Dovčiak, E. Kammoun |   Active galactic nuclei (AGNs) are known to be variable across all
wavelengths. Significant observational efforts have been invested in the last
decade in studying their ultraviolet (UV) and optical variability. Long and
densely sampled, multi-wavelength monitoring campaigns of numerous Seyfert
galaxies have been conducted with the aim of determining the X-ray/UV/optical
continuum time lags. Time-lag studies can be used to constrain theoretical
models. The observed time lags can be explained by thermal reprocessing of the
X-rays illuminating the accretion disc (known as the X-ray reverberation
model). However, the observed light curves contain more information that can be
used to further constrain physical models. Our primary objective is to
investigate whether, in addition to time lags, the X-ray reverberation model
can also explain the UV/optical variability amplitude of nearby Seyferts. To do
this, we measured the excess variance of four sources (namely Mrk 509, NGC
4151, NGC 2617, and Mrk 142) as a function of wavelength using data from
archival long, multi-wavelength campaigns with Swift, and ground-based
telescopes. We also computed the model excess variance in the case of the X-ray
reverberation model by determining the disc's transfer function and assuming a
bending power law for the X-ray power spectrum. We tested the validity of the
model by comparing the measured and model variances for a range of accretion
rates and X-ray source heights. We conclude that the X-ray thermal
reverberation model can fit both the continuum, UV/optical time lags, as well
as the variance in these AGNs, for the same physical parameters. Our results
suggest that the accretion disc is constant and that all the observed
UV/optical variations, on timescales of days and up to a few weeks, can be
fully explained by the variable X-rays as they illuminate the accretion disc.
 | General | [Link](http://arxiv.org/abs/2409.10417v1) |
| Local central limit theorem for Mallows measure | Alexey Bufetov, Kailun Chen |   We study the statistics of the Mallows measure on permutations in the limit
pioneered by Starr (2009). Our main result is the local central limit theorem
for its height function. We also re-derive versions of the law of large numbers
and the large deviation principle, obtain the standard central limit theorem
from the local one, and establish a multi-point version of the local central
limit theorem.
 | General | [Link](http://arxiv.org/abs/2409.10415v1) |
| On interactive anisotropic walks in two dimensions generated from a
  three state opinion dynamics model | Surajit Saha, Parongama Sen |   A system of interacting walkers on a two-dimensional space where the dynamics
of each walker are governed by the opinions of agents of a three-state opinion
dynamics model are considered. Such walks, inspired by Ising-like models and
opinions dynamics models, are usually considered in one-dimensional virtual
spaces. Here, the mapping is done in such a way that the walk is directed along
the $y$ axis while it can move either way along the $x$ axis. We explore the
properties of such walks as the parameter representing the noise in the opinion
dynamics model, responsible for a continuous phase transition, is varied. The
walk features show marked differences as the system crosses the critical point.
The bivariate distribution of the displacements below the critical point is a
modified biased Gaussian function of x and y which is symmetric about the X
axis.
  The marginal probability distributions can be extracted and the scaling forms
of different quantities, showing power law behaviour, are obtained. The
directed nature of the walk is reflected in the marginal distributions as well
as in the exponents.
 | General | [Link](http://arxiv.org/abs/2409.10413v1) |
| A Large-Scale Privacy Assessment of Android Third-Party SDKs | Mark Huasong Meng, Chuan Yan, Yun Hao, Qing Zhang, Zeyu Wang, Kailong Wang, Sin Gee Teo, Guangdong Bai, Jin Song Dong |   Third-party Software Development Kits (SDKs) are widely adopted in Android
app development, to effortlessly accelerate development pipelines and enhance
app functionality. However, this convenience raises substantial concerns about
unauthorized access to users' privacy-sensitive information, which could be
further abused for illegitimate purposes like user tracking or monetization.
Our study offers a targeted analysis of user privacy protection among Android
third-party SDKs, filling a critical gap in the Android software supply chain.
It focuses on two aspects of their privacy practices, including data
exfiltration and behavior-policy compliance (or privacy compliance), utilizing
techniques of taint analysis and large language models. It covers 158
widely-used SDKs from two key SDK release platforms, the official one and a
large alternative one. From them, we identified 338 instances of privacy data
exfiltration. On the privacy compliance, our study reveals that more than 30%
of the examined SDKs fail to provide a privacy policy to disclose their data
handling practices. Among those that provide privacy policies, 37% of them
over-collect user data, and 88% falsely claim access to sensitive data. We
revisit the latest versions of the SDKs after 12 months. Our analysis
demonstrates a persistent lack of improvement in these concerning trends. Based
on our findings, we propose three actionable recommendations to mitigate the
privacy leakage risks and enhance privacy protection for Android users. Our
research not only serves as an urgent call for industry attention but also
provides crucial insights for future regulatory interventions.
 | General | [Link](http://arxiv.org/abs/2409.10411v1) |
| A point process approach for the classification of noisy calcium imaging
  data | Arianna Burzacchi, Nicoletta D'Angelo, David Payares-Garcia, Jorge Mateu |   We study noisy calcium imaging data, with a focus on the classification of
spike traces. As raw traces obscure the true temporal structure of neuron's
activity, we performed a tuned filtering of the calcium concentration using two
methods: a biophysical model and a kernel mapping. The former characterizes
spike trains related to a particular triggering event, while the latter filters
out the signal and refines the selection of the underlying neuronal response.
Transitioning from traditional time series analysis to point process theory,
the study explores spike-time distance metrics and point pattern prototypes to
describe repeated observations. We assume that the analyzed neuron's firing
events, i.e. spike occurrences, are temporal point process events. In
particular, the study aims to categorize 47 point patterns by depth, assuming
the similarity of spike occurrences within specific depth categories. The
results highlight the pivotal roles of depth and stimuli in discerning diverse
temporal structures of neuron firing events, confirming the point process
approach based on prototype analysis is largely useful in the classification of
spike traces.
 | General | [Link](http://arxiv.org/abs/2409.10409v1) |
| Variational closures for composite homogenised fluid flows | Theo Diamantakis, Ruiao Hu |   The Stochastic Advection by Lie Transport is a variational formulation of
stochastic fluid dynamics introduced to model the effects of unresolved scales,
whilst preserving the geometric structure of ideal fluid flows. In this work,
we show that the SALT equations can arise from the decomposition of the fluid
flow map into its mean and fluctuating components. The fluctuating component is
realised as a prescribed stochastic diffeomorphism that introduces stochastic
transport into the system and we construct it using homogenisation theory. The
dynamics of the mean component are derived from a variational principle
utilising particular forms of variations that preserve the composite structure
of the flow. Using a new variational principle, we show that SALT equations can
arise from random Lagrangians and are equivalent to random coefficient PDEs. We
also demonstrate how to modify the composite flow and the associated
variational principle to derive models inspired by the Lagrangian Averaged
Euler-Poincare (LAEP) theory.
 | General | [Link](http://arxiv.org/abs/2409.10408v1) |
| Bitcoin Transaction Behavior Modeling Based on Balance Data | Yu Zhang, Claudio Tessone |   When analyzing Bitcoin users' balance distribution, we observed that it
follows a log-normal pattern. Drawing parallels from the successful application
of Gibrat's law of proportional growth in explaining city size and word
frequency distributions, we tested whether the same principle could account for
the log-normal distribution in Bitcoin balances. However, our calculations
revealed that the exponent parameters in both the drift and variance terms
deviate slightly from one. This suggests that Gibrat's proportional growth rule
alone does not fully explain the log-normal distribution observed in Bitcoin
users' balances. During our exploration, we discovered an intriguing
phenomenon: Bitcoin users tend to fall into two distinct categories based on
their behavior, which we refer to as ``poor" and ``wealthy" users. Poor users,
who initially purchase only a small amount of Bitcoin, tend to buy more
bitcoins first and then sell out all their holdings gradually over time. The
certainty of selling all their coins is higher and higher with time. In
contrast, wealthy users, who acquire a large amount of Bitcoin from the start,
tend to sell off their holdings over time. The speed at which they sell their
bitcoins is lower and lower over time and they will hold at least a small part
of their initial holdings at last. Interestingly, the wealthier the user, the
larger the proportion of their balance and the higher the certainty they tend
to sell. This research provided an interesting perspective to explore bitcoin
users' behaviors which may apply to other finance markets.
 | General | [Link](http://arxiv.org/abs/2409.10407v1) |
| Stochastic Data-Driven Predictive Control: Chance-Constraint
  Satisfaction with Identified Multi-step Predictors | Haldun Balim, Andrea Carron, Melanie N. Zeilinger, Johannes Köhler |   We propose a novel data-driven stochastic model predictive control framework
for uncertain linear systems with noisy output measurements. Our approach
leverages multi-step predictors to efficiently propagate uncertainty, ensuring
chance constraint satisfaction with minimal conservatism. In particular, we
present a strategy to identify multi-step predictors and quantify the
associated uncertainty using a surrogate (data-driven) state space model. Then,
we utilize the derived distribution to formulate a constraint tightening that
ensures chance constraint satisfaction. A numerical example highlights the
reduced conservatism of handling parametric uncertainty in the proposed method
compared to state-of-the-art solutions.
 | General | [Link](http://arxiv.org/abs/2409.10405v1) |
| A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning
  and BERT Integration | Zhang Zheng |   This paper proposes a knowledge-enhanced disease diagnosis method based on a
prompt learning framework. The method retrieves structured knowledge from
external knowledge graphs related to clinical cases, encodes it, and injects it
into the prompt templates to enhance the language model's understanding and
reasoning capabilities for the task.We conducted experiments on three public
datasets: CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR. The results show that the
proposed method significantly outperforms existing models across multiple
evaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTC
dataset, 3.1% on the IMCS-V2-NER dataset,and 4.2% on the KUAKE-QTR dataset.
Additionally,ablation studies confirmed the critical role of the knowledge
injection module,as the removal of this module resulted in a significant drop
in F1 score. The experimental results demonstrate that the proposed method not
only effectively improves the accuracy of disease diagnosis but also enhances
the interpretability of the predictions, providing more reliable support and
evidence for clinical diagnosis.
 | General | [Link](http://arxiv.org/abs/2409.10403v1) |
| A Statistical Equilibrium Approach to Adam Smith's Labor Theory of Value | Ellis Scharfenaker, Bruno Theodosio, Duncan K. Foley |   Adam Smith's inquiry into the emergence and stability of the
self-organization of the division of labor in commodity production and exchange
is considered using statistical equilibrium methods from statistical physics.
We develop a statistical equilibrium model of the distribution of independent
direct producers in a hub-and-spoke framework that predicts both the center of
gravity of producers across lines of production as well as the endogenous
fluctuations between lines of production that arise from Smith's concept of
"perfect liberty". The ergodic distribution of producers implies a long-run
balancing of "advantages to disadvantages" across lines of employment and
gravitation of market prices around Smith's natural prices.
 | General | [Link](http://arxiv.org/abs/2409.10402v1) |
| Hydrodynamic mechanism for stable spindle positioning in meiosis II
  oocytes | Weida Liao, Eric Lauga |   Cytoplasmic streaming, the persistent flow of fluid inside a cell, induces
intracellular transport, which plays a key role in fundamental biological
processes. In meiosis II mouse oocytes (developing egg cells) awaiting
fertilisation, the spindle, which is the protein structure responsible for
dividing genetic material in a cell, must maintain its position near the cell
cortex (the thin actin network bound to the cell membrane) for many hours.
However, the cytoplasmic streaming that accompanies this stable positioning
would intuitively appear to destabilise the spindle position. Here, through a
combination of numerical and analytical modelling, we reveal a new,
hydrodynamic mechanism for stable spindle positioning beneath the cortical cap.
We show that this stability depends critically on the spindle size and the
active driving from the cortex, and demonstrate that stable spindle positioning
can result purely from a hydrodynamic suction force exerted on the spindle by
the cytoplasmic flow. Our findings show that local fluid dynamic forces can be
sufficient to stabilise the spindle, explaining robustness against
perturbations not only perpendicular but also parallel to the cortex. Our
results shed light on the importance of cytoplasmic streaming in mammalian
meiosis.
 | General | [Link](http://arxiv.org/abs/2409.10401v1) |
| Sherpa: An Open Source Python Fitting Package | Aneta Siemiginowska, Douglas Burke, Hans Moritz Günther, Nicholas P. Lee, Warren McLaughlin, David A. Principe, Harlan Cheer, Antonella Fruscione, Omar Laurino, Jonathan McDowell, Marie Terrell |   We present an overview of Sherpa, an open source Python project, and discuss
its development history, broad design concepts and capabilities. Sherpa
contains powerful tools for combining parametric models into complex
expressions that can be fit to data using a variety of statistics and
optimization methods. It is easily extensible to include user-defined models,
statistics, and optimization methods. It provides a high-level User Interface
for interactive data-analysis, such as within a Jupyter notebook, and it can
also be used as a library component, providing fitting and modeling
capabilities to an application. We include a few examples of Sherpa
applications to multiwavelength astronomical data. The code is available
GitHub: https://github.com/sherpa/sherpa
 | General | [Link](http://arxiv.org/abs/2409.10400v1) |
| Minimally coupled $β$-exponential inflation with an $R^2$ term in
  the Palatini formulation | Nilay Bostan, Rafid H. Dejrah |   We focus on the inflationary predictions of $\beta$-exponential potential
models, in which the inflaton is a representation of the field delineating the
size of extra-dimension. Since it offers a well-motivated starting point for
the study of physics at very high energies, we incorporate an $R^2$ term in the
Palatini gravity. In addition, afterward the inflation, the inflaton oscillates
about the minimum of the inflation potential, and reheats the universe. This
occurs during the reheating phase, at which the inflaton decays into the
standard model particles, which fill the universe. We extend our examination by
considering the reheating effects on inflationary observables by employing the
different scenarios of the reheat temperature. Supposing the standard thermal
history after inflation, we display the inflationary predictions, $n_s, r,
\mathrm{d}n_s/\mathrm{d}\ln k$ of $\beta$-exponential potential with minimal
coupling in Palatini $R^2$ gravity. Also, different kinds of constraints from a
variety of observations, such as BICEP/Keck, Planck 2018, as well as future
possible detectable sensitivities that might be reached by CMB experiments:
CMB-S4 and LiteBIRD are taken into account in this work. We indicate that our
results are consistent with both the latest data and the future sensitivity
forecasts of LiteBIRD/Planck and CMB-S4. Finally, the results in this study
highlight the viability of our model even in the case of the existence of more
stringent constraints expected from future achievable confidence level limits.
 | General | [Link](http://arxiv.org/abs/2409.10398v1) |
| Quantifying Observational Projection Effects with a Simulation-based hot
  CGM model | Soumya Shreeram, Johan Comparat, Andrea Merloni, Yi Zhang, Gabriele Ponti, Kirpal Nandra, John ZuHone, Ilaria Marini, Stephan Vladutescu-Zopp, Paola Popesso, Ruediger Pakmor, Riccardo Seppi, Celine Peroux, Daniele Sorini |   The hot phase of the circumgalactic medium (CGM) allows us to probe the
inflow and outflow of gas within a galaxy, which is responsible for dictating
the evolution of the galaxy. Studying the hot CGM sheds light on a better
understanding of gas physics, which is crucial to inform and constrain
simulation models. With the recent advances in observational measurements
probing the hot CGM in X-rays and tSZ, we have a new avenue for widening our
knowledge of gas physics and feedback by exploiting the information from
current/future observations. In this paper, we use the TNG300 hydrodynamical
simulations to build a fully self-consistent forward model for the hot CGM. We
construct a lightcone and generate mock X-ray observations. We quantify the
projection effects, namely the locally correlated large-scale structure in
X-rays and the effect due to satellite galaxies misclassified as centrals which
affects the measured hot CGM galactocentric profiles in stacking experiments.
We present an analytical model that describes the intrinsic X-ray surface
brightness profile across the stellar and halo mass bins. The increasing
stellar mass bins result in decreasing values of $\beta$, the exponent
quantifying the slope of the intrinsic galactocentric profiles. We carry
forward the current state-of-the-art by also showing the impact of the locally
correlated environment on the measured X-ray surface brightness profiles. We
also present, for the first time, the effect of misclassified centrals in
stacking experiments for three stellar mass bins: $10^{10.5-11}\ M_\odot$,
$10^{11-11.2}\ M_\odot$, and $10^{11.2-11.5}\ M_\odot$. We find that the
contaminating effect of the misclassified centrals on the stacked profiles
increases when the stellar mass decreases.
 | General | [Link](http://arxiv.org/abs/2409.10397v1) |
| MOST: MR reconstruction Optimization for multiple downStream Tasks via
  continual learning | Hwihun Jeong, Se Young Chun, Jongho Lee |   Deep learning-based Magnetic Resonance (MR) reconstruction methods have
focused on generating high-quality images but they often overlook the impact on
downstream tasks (e.g., segmentation) that utilize the reconstructed images.
Cascading separately trained reconstruction network and downstream task network
has been shown to introduce performance degradation due to error propagation
and domain gaps between training datasets. To mitigate this issue, downstream
task-oriented reconstruction optimization has been proposed for a single
downstream task. Expanding this optimization to multi-task scenarios is not
straightforward. In this work, we extended this optimization to sequentially
introduced multiple downstream tasks and demonstrated that a single MR
reconstruction network can be optimized for multiple downstream tasks by
deploying continual learning (MOST). MOST integrated techniques from
replay-based continual learning and image-guided loss to overcome catastrophic
forgetting. Comparative experiments demonstrated that MOST outperformed a
reconstruction network without finetuning, a reconstruction network with
na\"ive finetuning, and conventional continual learning methods. This
advancement empowers the application of a single MR reconstruction network for
multiple downstream tasks. The source code is available at:
https://github.com/SNU-LIST/MOST
 | General | [Link](http://arxiv.org/abs/2409.10394v1) |
| Multicopy quantum state teleportation with application to storage and
  retrieval of quantum programs | Frédéric Grosshans, Michał Horodecki, Mio Murao, Tomasz Młynik, Marco Túlio Quintino, Michał Studziński, Satoshi Yoshida |   This work considers a teleportation task for Alice and Bob in a scenario
where Bob cannot perform corrections. In particular, we analyse the task of
\textit{multicopy state teleportation}, where Alice has $k$ identical copies of
an arbitrary unknown $d$-dimensional qudit state $\vert\psi\rangle$ to teleport
a single copy of $\vert\psi\rangle$ to Bob using a maximally entangled
two-qudit state shared between Alice and Bob without Bob's correction. Alice
may perform a joint measurement on her half of the entangled state and the $k$
copies of $\vert\psi\rangle$. We prove that the maximal probability of success
for teleporting the exact state $\vert\psi\rangle$ to Bob is
$p(d,k)=\frac{k}{d(k-1+d)}$ and present an explicit protocol to attain this
performance. Then, by utilising $k$ copies of an arbitrary target state
$\vert\psi\rangle$, we show how the multicopy state teleportation protocol can
be employed to enhance the success probability of storage and retrieval of
quantum programs, which aims to universally retrieve the action of an arbitrary
quantum channel that is stored in a state. Our proofs make use of group
representation theory methods, which may find applications beyond the problems
addressed in this work.
 | General | [Link](http://arxiv.org/abs/2409.10393v1) |
| TPFL: Tsetlin-Personalized Federated Learning with Confidence-Based
  Clustering | Rasoul Jafari Gohari, Laya Aliahmadipour, Ezat Valipour |   The world of Machine Learning (ML) has witnessed rapid changes in terms of
new models and ways to process users data. The majority of work that has been
done is focused on Deep Learning (DL) based approaches. However, with the
emergence of new algorithms such as the Tsetlin Machine (TM) algorithm, there
is growing interest in exploring alternative approaches that may offer unique
advantages in certain domains or applications. One of these domains is
Federated Learning (FL), in which users privacy is of utmost importance. Due to
its novelty, FL has seen a surge in the incorporation of personalization
techniques to enhance model accuracy while maintaining user privacy under
personalized conditions. In this work, we propose a novel approach dubbed TPFL:
Tsetlin-Personalized Federated Learning, in which models are grouped into
clusters based on their confidence towards a specific class. In this way,
clustering can benefit from two key advantages. Firstly, clients share only
what they are confident about, resulting in the elimination of wrongful weight
aggregation among clients whose data for a specific class may have not been
enough during the training. This phenomenon is prevalent when the data are
non-Independent and Identically Distributed (non-IID). Secondly, by sharing
only weights towards a specific class, communication cost is substantially
reduced, making TPLF efficient in terms of both accuracy and communication
cost. The results of TPFL demonstrated the highest accuracy on three different
datasets; namely MNIST, FashionMNIST and FEMNIST.
 | General | [Link](http://arxiv.org/abs/2409.10392v1) |
| Prompt-and-Transfer: Dynamic Class-aware Enhancement for Few-shot
  Segmentation | Hanbo Bi, Yingchao Feng, Wenhui Diao, Peijin Wang, Yongqiang Mao, Kun Fu, Hongqi Wang, Xian Sun |   For more efficient generalization to unseen domains (classes), most Few-shot
Segmentation (FSS) would directly exploit pre-trained encoders and only
fine-tune the decoder, especially in the current era of large models. However,
such fixed feature encoders tend to be class-agnostic, inevitably activating
objects that are irrelevant to the target class. In contrast, humans can
effortlessly focus on specific objects in the line of sight. This paper mimics
the visual perception pattern of human beings and proposes a novel and powerful
prompt-driven scheme, called ``Prompt and Transfer" (PAT), which constructs a
dynamic class-aware prompting paradigm to tune the encoder for focusing on the
interested object (target class) in the current task. Three key points are
elaborated to enhance the prompting: 1) Cross-modal linguistic information is
introduced to initialize prompts for each task. 2) Semantic Prompt Transfer
(SPT) that precisely transfers the class-specific semantics within the images
to prompts. 3) Part Mask Generator (PMG) that works in conjunction with SPT to
adaptively generate different but complementary part prompts for different
individuals. Surprisingly, PAT achieves competitive performance on 4 different
tasks including standard FSS, Cross-domain FSS (e.g., CV, medical, and remote
sensing domains), Weak-label FSS, and Zero-shot Segmentation, setting new
state-of-the-arts on 11 benchmarks.
 | General | [Link](http://arxiv.org/abs/2409.10389v1) |
| Revising the Structure of Recurrent Neural Networks to Eliminate
  Numerical Derivatives in Forming Physics Informed Loss Terms with Respect to
  Time | Mahyar Jahani-nasab, Mohamad Ali Bijarchi |   Solving unsteady partial differential equations (PDEs) using recurrent neural
networks (RNNs) typically requires numerical derivatives between each block of
the RNN to form the physics informed loss function. However, this introduces
the complexities of numerical derivatives into the training process of these
models. In this study, we propose modifying the structure of the traditional
RNN to enable the prediction of each block over a time interval, making it
possible to calculate the derivative of the output with respect to time using
the backpropagation algorithm. To achieve this, the time intervals of these
blocks are overlapped, defining a mutual loss function between them.
Additionally, the employment of conditional hidden states enables us to achieve
a unique solution for each block. The forget factor is utilized to control the
influence of the conditional hidden state on the prediction of the subsequent
block. This new model, termed the Mutual Interval RNN (MI-RNN), is applied to
solve three different benchmarks: the Burgers equation, unsteady heat
conduction in an irregular domain, and the Green vortex problem. Our results
demonstrate that MI-RNN can find the exact solution more accurately compared to
existing RNN models. For instance, in the second problem, MI-RNN achieved one
order of magnitude less relative error compared to the RNN model with numerical
derivatives.
 | General | [Link](http://arxiv.org/abs/2409.10388v1) |
| Sharp decay rate for eigenfunctions of perturbed periodic Schrödinger
  operators | Wencai Liu, Rodrigo Matos, John N. Treuer |   This paper investigates uniqueness results for perturbed periodic
Schr\"odinger operators on $\mathbb{Z}^d$. Specifically, we consider operators
of the form $H = -\Delta + V + v$, where $\Delta$ is the discrete Laplacian,
$V: \mathbb{Z}^d \rightarrow \mathbb{R}$ is a periodic potential, and $v:
\mathbb{Z}^d \rightarrow \mathbb{C}$ represents a decaying impurity. We
establish quantitative conditions under which the equation $-\Delta u + V u + v
u = \lambda u$, for $\lambda \in \mathbb{C}$, admits only the trivial solution
$u \equiv 0$. Key applications include the absence of embedded eigenvalues for
operators with impurities decaying faster than any exponential function and the
determination of sharp decay rates for eigenfunctions. Our findings extend
previous works by providing precise decay conditions for impurities and
analyzing different spectral regimes of $\lambda$.
 | General | [Link](http://arxiv.org/abs/2409.10387v1) |
| Mamba-ST: State Space Model for Efficient Style Transfer | Filippo Botti, Alex Ergasti, Leonardo Rossi, Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi, Andrea Prati |   The goal of style transfer is, given a content image and a style source,
generating a new image preserving the content but with the artistic
representation of the style source. Most of the state-of-the-art architectures
use transformers or diffusion-based models to perform this task, despite the
heavy computational burden that they require. In particular, transformers use
self- and cross-attention layers which have large memory footprint, while
diffusion models require high inference time. To overcome the above, this paper
explores a novel design of Mamba, an emergent State-Space Model (SSM), called
Mamba-ST, to perform style transfer. To do so, we adapt Mamba linear equation
to simulate the behavior of cross-attention layers, which are able to combine
two separate embeddings into a single output, but drastically reducing memory
usage and time complexity. We modified the Mamba's inner equations so to accept
inputs from, and combine, two separate data streams. To the best of our
knowledge, this is the first attempt to adapt the equations of SSMs to a vision
task like style transfer without requiring any other module like
cross-attention or custom normalization layers. An extensive set of experiments
demonstrates the superiority and efficiency of our method in performing style
transfer compared to transformers and diffusion models. Results show improved
quality in terms of both ArtFID and FID metrics. Code is available at
https://github.com/FilippoBotti/MambaST.
 | General | [Link](http://arxiv.org/abs/2409.10385v1) |
| Performance-Portable Numerical Relativity with AthenaK | Hengrui Zhu, Jacob Fields, Francesco Zappa, David Radice, James Stone, Alireza Rashti, William Cook, Sebastiano Bernuzzi, Boris Daszuta |   We present the numerical relativity module within AthenaK, an open source
performance-portable astrophysics code designed for exascale computing
applications. This module employs the Z4c formulation to solve the Einstein
equations. We demonstrate its accuracy through a series of standard numerical
relativity tests, including convergence of the gravitational waveform from
binary black hole coalescence. Furthermore, we conduct scaling tests on OLCF
Frontier and NERSC Perlmutter, where AthenaK exhibits excellent weak scaling
efficiency of 80% on up to 65,536 AMD MI250X GPUs on Frontier (relative to 4
GPUs) and strong scaling efficiencies of 84% and 77% on AMD MI250X and NVIDIA
A100 GPUs on Frontier and Perlmutter respectively. Additionally, we observe a
significant performance boost, with two orders of magnitude speedup ($\gtrsim
200\times$) on a GPU compared to a single CPU core, affirming that AthenaK is
well-suited for exascale computing, thereby expanding the potential for
breakthroughs in numerical relativity research.
 | General | [Link](http://arxiv.org/abs/2409.10383v1) |
| J-UBIK: The JAX-accelerated Universal Bayesian Imaging Kit | Vincent Eberle, Matteo Guardiani, Margret Westerkamp, Philipp Frank, Julian Rüstig, Julia Stadler, Torsten A. Enßlin |   Many advances in astronomy and astrophysics originate from accurate images of
the sky emission across multiple wavelengths. This often requires
reconstructing spatially and spectrally correlated signals detected from
multiple instruments. To facilitate the high-fidelity imaging of these signals,
we introduce the universal Bayesian imaging kit (UBIK). Specifically, we
present J-UBIK, a flexible and modular implementation leveraging the
JAX-accelerated NIFTy.re software as its backend. J-UBIK streamlines the
implementation of the key Bayesian inference components, providing for all the
necessary steps of Bayesian imaging pipelines. First, it provides adaptable
prior models for different sky realizations. Second, it includes likelihood
models tailored to specific instruments. So far, the package includes three
instruments: Chandra and eROSITA for X-ray observations, and the James Webb
Space Telescope (JWST) for the near- and mid-infrared. The aim is to expand
this set in the future. Third, these models can be integrated with various
inference and optimization schemes, such as maximum a posteriori estimation and
variational inference. Explicit demos show how to integrate the individual
modules into a full analysis pipeline. Overall, J-UBIK enables efficient
generation of high-fidelity images via Bayesian pipelines that can be tailored
to specific research objectives.
 | General | [Link](http://arxiv.org/abs/2409.10381v1) |
| Thermolectricity in irradiated bilayer graphene flakes | Cynthia Ihuoma Osuala, Tanu Choudhary, Raju K. Biswas, Sudin Ganguly, Chunlei Qu, Santanu K. Maiti |   We present a comprehensive study on enhancing the thermoelectric (TE)
performance of bilayer graphene (BLG) through irradiation with arbitrarily
polarized light, focusing on $AA$- and $AB$-stacked configurations with zigzag
edges. Utilizing a combination of tight-binding theory and density functional
theory (DFT), we systematically analyze the impact of light irradiation on
electronic and phononic transport properties. Light irradiation alters the
electronic hopping parameters, creating an asymmetric transmission function,
which significantly increases the Seebeck coefficient, thereby boosting the
overall {\it figure of merit} (FOM). For the phononic contribution, DFT
calculations reveal that $AB$-stacked BLG exhibits lower lattice thermal
conductivity compared to $AA$-stacked, attributed to enhanced anharmonic
scattering and phonon group velocity. The combined analysis shows that FOM
exceeds unity in both stacking types, with notable improvements near the
irradiation-induced gap. Additionally, we explore the dependence of FOM on the
system dimensions and temperature, demonstrating that light-irradiated BLG
holds great promise for efficient thermoelectric energy conversion and waste
heat recovery. Our results show favorable responses over a wide range of
irradiation parameters. These findings provide crucial insights into optimizing
BLG for advanced TE applications through light-induced modifications.
 | General | [Link](http://arxiv.org/abs/2409.10380v1) |
| Leveraging Joint Spectral and Spatial Learning with MAMBA for
  Multichannel Speech Enhancement | Wenze Ren, Haibin Wu, Yi-Cheng Lin, Xuanjun Chen, Rong Chao, Kuo-Hsuan Hung, You-Jin Li, Wen-Yuan Ting, Hsin-Min Wang, Yu Tsao |   In multichannel speech enhancement, effectively capturing spatial and
spectral information across different microphones is crucial for noise
reduction. Traditional methods, such as CNN or LSTM, attempt to model the
temporal dynamics of full-band and sub-band spectral and spatial features.
However, these approaches face limitations in fully modeling complex temporal
dependencies, especially in dynamic acoustic environments. To overcome these
challenges, we modify the current advanced model McNet by introducing an
improved version of Mamba, a state-space model, and further propose MCMamba.
MCMamba has been completely reengineered to integrate full-band and narrow-band
spatial information with sub-band and full-band spectral features, providing a
more comprehensive approach to modeling spatial and spectral information. Our
experimental results demonstrate that MCMamba significantly improves the
modeling of spatial and spectral features in multichannel speech enhancement,
outperforming McNet and achieving state-of-the-art performance on the CHiME-3
dataset. Additionally, we find that Mamba performs exceptionally well in
modeling spectral information.
 | General | [Link](http://arxiv.org/abs/2409.10376v1) |
| Nonlinear Causality in Brain Networks: With Application to Motor Imagery
  vs Execution | Sipan Aslan, Hernando Ombao |   One fundamental challenge of data-driven analysis in neuroscience is modeling
causal interactions and exploring the connectivity of nodes in a brain network.
Various statistical methods, relying on various perspectives and employing
different data modalities, are being developed to examine and comprehend the
underlying causal structures inherent to brain dynamics. This study introduces
a novel statistical approach, TAR4C, to dissect causal interactions in
multichannel EEG recordings. TAR4C uses the threshold autoregressive model to
describe the causal interaction between nodes or clusters of nodes in a brain
network. The perspective involves testing whether one node, which may represent
a brain region, can control the dynamics of the other. The node that has such
an impact on the other is called a threshold variable and can be classified as
a causative because its functionality is the leading source operating as an
instantaneous switching mechanism that regulates the time-varying
autoregressive structure of the other. This statistical concept is commonly
referred to as threshold non-linearity. Once threshold non-linearity has been
verified between a pair of nodes, the subsequent essential facet of TAR
modeling is to assess the predictive ability of the causal node for the current
activity on the other and represent causal interactions in autoregressive
terms. This predictive ability is what underlies Granger causality. The TAR4C
approach can discover non-linear and time-dependent causal interactions without
negating the G-causality perspective. The efficacy of the proposed approach is
exemplified by analyzing the EEG signals recorded during the motor
movement/imagery experiment. The similarities and differences between the
causal interactions manifesting during the execution and the imagery of a given
motor movement are demonstrated by analyzing EEG recordings from multiple
subjects.
 | General | [Link](http://arxiv.org/abs/2409.10374v1) |
| Instigating Cooperation among LLM Agents Using Adaptive Information
  Modulation | Qiliang Chen,  Alireza,  Ilami, Nunzio Lore, Babak Heydari |   This paper introduces a novel framework combining LLM agents as proxies for
human strategic behavior with reinforcement learning (RL) to engage these
agents in evolving strategic interactions within team environments. Our
approach extends traditional agent-based simulations by using strategic LLM
agents (SLA) and introducing dynamic and adaptive governance through a
pro-social promoting RL agent (PPA) that modulates information access across
agents in a network, optimizing social welfare and promoting pro-social
behavior. Through validation in iterative games, including the prisoner
dilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.
The PPA agent effectively learns to adjust information transparency, resulting
in enhanced cooperation rates. This framework offers significant insights into
AI-mediated social dynamics, contributing to the deployment of AI in real-world
team settings.
 | General | [Link](http://arxiv.org/abs/2409.10372v1) |
| Learning Gentle Grasping from Human-Free Force Control Demonstration | Mingxuan Li, Lunwei Zhang, Tiemin Li, Yao Jiang |   Humans can steadily and gently grasp unfamiliar objects based on tactile
perception. Robots still face challenges in achieving similar performance due
to the difficulty of learning accurate grasp-force predictions and force
control strategies that can be generalized from limited data. In this article,
we propose an approach for learning grasping from ideal force control
demonstrations, to achieve similar performance of human hands with limited data
size. Our approach utilizes objects with known contact characteristics to
automatically generate reference force curves without human demonstrations. In
addition, we design the dual convolutional neural networks (Dual-CNN)
architecture which incorporating a physics-based mechanics module for learning
target grasping force predictions from demonstrations. The described method can
be effectively applied in vision-based tactile sensors and enables gentle and
stable grasping of objects from the ground. The described prediction model and
grasping strategy were validated in offline evaluations and online experiments,
and the accuracy and generalizability were demonstrated.
 | General | [Link](http://arxiv.org/abs/2409.10371v1) |
| Uncovering the Mechanism of Hepatotoxiciy of PFAS Targeting L-FABP Using
  GCN and Computational Modeling | Lucas Jividen, Tibo Duran, Xi-Zhi Niu, Jun Bai |   Per- and polyfluoroalkyl substances (PFAS) are persistent environmental
pollutants with known toxicity and bioaccumulation issues. Their widespread
industrial use and resistance to degradation have led to global environmental
contamination and significant health concerns. While a minority of PFAS have
been extensively studied, the toxicity of many PFAS remains poorly understood
due to limited direct toxicological data. This study advances the predictive
modeling of PFAS toxicity by combining semi-supervised graph convolutional
networks (GCNs) with molecular descriptors and fingerprints. We propose a novel
approach to enhance the prediction of PFAS binding affinities by isolating
molecular fingerprints to construct graphs where then descriptors are set as
the node features. This approach specifically captures the structural,
physicochemical, and topological features of PFAS without overfitting due to an
abundance of features. Unsupervised clustering then identifies representative
compounds for detailed binding studies. Our results provide a more accurate
ability to estimate PFAS hepatotoxicity to provide guidance in chemical
discovery of new PFAS and the development of new safety regulations.
 | General | [Link](http://arxiv.org/abs/2409.10370v1) |
| Stochastic Control of UAVs: An Optimal Tradeoff between Performance,
  Flight Smoothness and Control Effort | George Rapakoulias, Panagiotis Tsiotras |   Safe and accurate control of unmanned aerial vehicles in the presence of
winds is a challenging control problem due to the hard-to-model and highly
stochastic nature of the disturbance forces acting upon the vehicle. To meet
performance constraints, state-of-the-art control methods such as Incremental
Nonlinear Dynamic Inversion (INDI) or other adaptive control techniques require
high control gains to mitigate the effects of uncertainty entering the system.
While achieving good tracking performance, IDNI requires excessive control
effort, results in high actuator strain, and reduced flight smoothness due to
constant and aggressive corrective actions commanded by the controller. In this
paper, we propose a novel control architecture that allows the user to
systematically address the trade-off between high authority control and
performance constraint satisfaction. Our approach consists of two parts. To
cancel out biases introduced by unmodelled aerodynamic effects we propose a
hybrid, model-based disturbance force estimator augmented with a neural
network, that can adapt to external wind conditions using a Kalman Filter. We
then utilize state-of-the-art results from Covariance Steering theory, which
offers a principled way of controlling the uncertainty of the tracking error
dynamics. We first analyze the properties of the combined system and then
provide extensive experimental results to verify the advantages of the proposed
approach over existing methods
 | General | [Link](http://arxiv.org/abs/2409.10369v1) |
| Integrated nowcasting of convective precipitation with Transformer-based
  models using multi-source data | Çağlar Küçük, Aitor Atencia, Markus Dabernig |   Precipitation nowcasting is crucial for mitigating the impacts of severe
weather events and supporting daily activities. Conventional models
predominantly relying on radar data have limited performance in predicting
cases with complex temporal features such as convection initiation,
highlighting the need to integrate data from other sources for more
comprehensive nowcasting. Unlike physics-based models, machine learning
(ML)-based models offer promising solutions for efficiently integrating large
volumes of diverse data. We present EF4INCA, a spatiotemporal Transformer model
for precipitation nowcasting that integrates satellite- and ground-based
observations with numerical weather prediction outputs. EF4INCA provides
high-resolution forecasts over Austria, accurately predicting the location and
shape of precipitation fields with a spatial resolution of 1 kilometre and a
temporal resolution of 5 minutes, up to 90 minutes ahead. Our evaluation shows
that EF4INCA outperforms conventional nowcasting models, including the
operational model of Austria, particularly in scenarios with complex temporal
features such as convective initiation and rapid weather changes. EF4INCA
maintains higher accuracy in location forecasting but generates smoother fields
at later prediction times compared to traditional models. Interpretation of our
model showed that precipitation products and SEVIRI infrared channels CH7 and
CH9 are the most important data streams. These results underscore the
importance of combining data from different domains, including physics-based
model products, with ML approaches. Our study highlights the robustness of
EF4INCA and its potential for improved precipitation nowcasting. We provide
access to our code repository, model weights, and the dataset curated for
benchmarking, facilitating further development and application.
 | General | [Link](http://arxiv.org/abs/2409.10367v1) |



--------------------------------------------------------------------------------

Next speaker: Admin

Replying as Admin. Provide feedback to chat_manager. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: 